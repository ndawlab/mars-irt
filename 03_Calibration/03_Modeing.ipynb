{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82f4016f-87bd-41dd-90eb-e92afe20fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas import read_csv\n",
    "from arviz import hdi\n",
    "sns.set_theme(style='white', context='notebook', font_scale=1.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcec38db-a532-4a6f-aa3b-b64522e11210",
   "metadata": {},
   "source": [
    "## Section 1: 1PL-G Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d93689-ad90-47f9-8db8-28dcf07af6b0",
   "metadata": {},
   "source": [
    "### 1.1 Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5721451e-7a2d-482d-82ef-97097153831b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1plg_m1: -14516.85 | 93.29\n",
      "1plg_m2: -13804.74 | 159.38\n",
      "1plg_m3: -13481.12 | 164.26\n",
      "1plg_m4: -13293.16 | 339.87\n"
     ]
    }
   ],
   "source": [
    "for i, m in enumerate(['1plg_m1','1plg_m2','1plg_m3','1plg_m4']):\n",
    "    \n",
    "    ## Load ppc.\n",
    "    ppc = read_csv(os.path.join('stan_results', f'{m}_ppc.csv'))\n",
    "        \n",
    "    ## Model comparison.\n",
    "    loco = ppc.groupby('subject').loco.mean().sum()\n",
    "    pwaic_c = ppc.groupby('subject').pwaic_c.mean().sum()\n",
    "    \n",
    "    print('%s: %0.2f | %0.2f' %(m, loco, pwaic_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1387114-760b-4392-b426-6918aaabdedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1plg_m2 - 1plg_m1: 712.11 (34.61)\n",
      "1plg_m3 - 1plg_m1: 1035.73 (41.95)\n",
      "1plg_m4 - 1plg_m1: 1223.69 (44.94)\n",
      "1plg_m3 - 1plg_m2: 323.62 (23.36)\n",
      "1plg_m4 - 1plg_m2: 511.58 (28.77)\n",
      "1plg_m4 - 1plg_m3: 187.96 (17.25)\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "for a, b in list(combinations(['1plg_m1','1plg_m2','1plg_m3','1plg_m4'], 2)):\n",
    "    \n",
    "    ## Load data.\n",
    "    df1 = read_csv(os.path.join('stan_results', f'{a}_ppc.csv'))\n",
    "    df2 = read_csv(os.path.join('stan_results', f'{b}_ppc.csv'))\n",
    "    \n",
    "    arr = df2.groupby('subject').loco.mean() - df1.groupby('subject').loco.mean()\n",
    "    \n",
    "    ## Compute stats.\n",
    "    N = df1.subject.nunique()\n",
    "    mu = np.sum(arr)\n",
    "    se = np.std(arr) * np.sqrt(N)\n",
    "    \n",
    "    print(f'{b} - {a}: %0.2f (%0.2f)' %(mu, se))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc10e2d7-1b34-4e8b-a411-eea581f7e434",
   "metadata": {},
   "source": [
    "### 1.2 Parameter distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "889308e6-f595-4d20-9056-7fff56d349c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta (mean):\t0.177 [0.118, 0.238]\n",
      "beta (var):\t1.431 [1.370, 1.492]\n"
     ]
    }
   ],
   "source": [
    "## Load samples.\n",
    "samples = read_csv(os.path.join('stan_results', '1plg_m4.tsv.gz'), sep='\\t', compression='gzip')\n",
    "\n",
    "## Extract parameters.\n",
    "beta_mu   = samples.filter(regex='beta_mu\\[').values\n",
    "beta_1_pr = samples.filter(regex='beta_1_pr\\[').values\n",
    "beta      = samples.filter(regex='beta\\[').values\n",
    "sigma     = samples.filter(regex='sigma\\[').values\n",
    "\n",
    "## Item difficulty (mean).\n",
    "mu = beta.mean()\n",
    "lb, ub = hdi(beta.mean(axis=1), hdi_prob=0.95)\n",
    "print('beta (mean):\\t%0.3f [%0.3f, %0.3f]' %(mu, lb, ub))\n",
    "\n",
    "## Item difficulty (variance).\n",
    "mu = beta.std(axis=1).mean()\n",
    "lb, ub = hdi(beta.std(axis=1), hdi_prob=0.95)\n",
    "print('beta (var):\\t%0.3f [%0.3f, %0.3f]' %(mu, lb, ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43546293-f0a5-4a99-aa27-d02885832481",
   "metadata": {},
   "source": [
    "### 1.3 Interpreting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b4d8f3a-fecd-4b29-ab27-0ce19b02211d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_fea:\t0.132 [0.087, 0.174]\n",
      "n_rul:\t0.112 [0.080, 0.144]\n",
      "distr:\t0.252 [0.218, 0.287]\n",
      "rt:\t0.122 [0.093, 0.151]\n"
     ]
    }
   ],
   "source": [
    "## Define useful functions.\n",
    "def inv_logit(x):\n",
    "    return 1. / (1 + np.exp(-x))\n",
    "\n",
    "## Load design.\n",
    "X2 = read_csv(os.path.join('designs', 'X2.csv'))\n",
    "\n",
    "## Iterate over fixed effects.\n",
    "for i, col in enumerate(['n_features', 'n_rules', 'distractor', 'rt']):\n",
    "    \n",
    "    ## Compute regressor SD.\n",
    "    sd = X2[col].std()\n",
    "    \n",
    "    ## Compute change in response.\n",
    "    delta = inv_logit(beta_mu[:,0] + 0.5 * (beta_mu[:,i+1] / sd))  -\\\n",
    "            inv_logit(beta_mu[:,0] - 0.5 * (beta_mu[:,i+1] / sd))\n",
    "    \n",
    "    ## Compute summary stats.\n",
    "    mu = delta.mean()\n",
    "    lb, ub = hdi(delta, hdi_prob=0.95)\n",
    "    \n",
    "    ## Print.\n",
    "    print('%s:\\t%0.3f [%0.3f, %0.3f]' %(col[:5],mu,lb,ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba7750-7afa-47c5-903a-58cb7447d5fe",
   "metadata": {},
   "source": [
    "### 1.4 Variance explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe4f30e8-9759-4615-968b-811d5f017eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level-1   fixed:\t 0.350\n",
      "all item  fixed:\t 0.482\n",
      "level-1,2 fixed:\t 0.656\n",
      "all effects:    \t 0.841\n"
     ]
    }
   ],
   "source": [
    "## Load and prepare design matrices.\n",
    "zscore = lambda x: (x - np.nanmean(x)) / np.nanstd(x)\n",
    "X2 = read_csv(os.path.join('designs', 'X2.csv')).apply(zscore, 0).fillna(1).values\n",
    "X3 = read_csv(os.path.join('designs', 'X3.csv')).values.astype(float)\n",
    "\n",
    "## Compute total variance.\n",
    "total_var = np.var(beta, axis=1).mean()\n",
    "\n",
    "## Compute explained variance (level-1 fixed effects).\n",
    "var = np.var(beta - beta_mu[:,[1,2]] @ X2.T[[1,2]], axis=1).mean()\n",
    "print('level-1   fixed:\\t %0.3f' %(1 - var / total_var))\n",
    "\n",
    "## Compute explained variance (item fixed effects).\n",
    "var = np.var(beta - beta_mu[:,[1,2,3]] @ X2.T[[1,2,3]], axis=1).mean()\n",
    "print('all item  fixed:\\t %0.3f' %(1 - var / total_var))\n",
    "\n",
    "## Compute explained variance (level-1,2 fixed effects).\n",
    "var = np.var(beta - beta_mu @ X2.T, axis=1).mean()\n",
    "print('level-1,2 fixed:\\t %0.3f' %(1 - var / total_var))\n",
    "\n",
    "## Compute variance explained (all effects).\n",
    "mu = beta_mu @ X2.T + beta_1_pr @ X3.T * np.atleast_2d(sigma[:,0]).T\n",
    "var = np.var(beta - mu, axis=1).mean()\n",
    "print('all effects:    \\t %0.3f' %(1 - var / total_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2654d597-de26-4fa1-b473-9316456d286e",
   "metadata": {},
   "source": [
    "## Section 2: 3PL Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6f1a9c-943a-461b-9f52-f6cd738a0c33",
   "metadata": {},
   "source": [
    "### 2.1 Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e3171ff-00e0-488d-9d74-e9915b3b3d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3pl_m1: -13209.38 | 384.90\n",
      "3pl_m2: -13207.25 | 408.13\n",
      "3pl_m3: -13208.69 | 426.78\n"
     ]
    }
   ],
   "source": [
    "for i, m in enumerate(['3pl_m1','3pl_m2','3pl_m3']):\n",
    "    \n",
    "    ## Load ppc.\n",
    "    ppc = read_csv(os.path.join('stan_results', f'{m}_ppc.csv'))\n",
    "        \n",
    "    ## Model comparison.\n",
    "    loco = ppc.groupby('subject').loco.mean().sum()\n",
    "    pwaic_c = ppc.groupby('subject').pwaic_c.mean().sum()\n",
    "    \n",
    "    print('%s: %0.2f | %0.2f' %(m, loco, pwaic_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbff71aa-c119-41cc-9ebd-05529d82f4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3pl_m2 - 3pl_m1: 2.13 (2.04)\n",
      "3pl_m3 - 3pl_m1: 0.69 (2.20)\n",
      "3pl_m3 - 3pl_m2: -1.44 (0.85)\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "for a, b in list(combinations(['3pl_m1','3pl_m2','3pl_m3'], 2)):\n",
    "    \n",
    "    ## Load data.\n",
    "    df1 = read_csv(os.path.join('stan_results', f'{a}_ppc.csv'))\n",
    "    df2 = read_csv(os.path.join('stan_results', f'{b}_ppc.csv'))\n",
    "    \n",
    "    arr = df2.groupby('subject').loco.mean() - df1.groupby('subject').loco.mean()\n",
    "    \n",
    "    ## Compute stats.\n",
    "    N = df1.subject.nunique()\n",
    "    mu = np.sum(arr)\n",
    "    se = np.std(arr) * np.sqrt(N)\n",
    "    \n",
    "    print(f'{b} - {a}: %0.2f (%0.2f)' %(mu, se))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5c0ceb-777f-4f85-86a8-8df1d0d1c797",
   "metadata": {},
   "source": [
    "### 2.2 Parameter distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "625591a0-cfd4-4cef-9c8b-12fa8181481b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta (mean):\t0.221 [0.142, 0.302]\n",
      "beta (var):\t1.554 [1.475, 1.629]\n",
      "alpha (mean):\t1.298 [1.212, 1.385]\n",
      "alpha (var):\t0.221 [1.475, 1.629]\n"
     ]
    }
   ],
   "source": [
    "## Load samples.\n",
    "samples = read_csv(os.path.join('stan_results', '3pl_m2.tsv.gz'), sep='\\t', compression='gzip')\n",
    "\n",
    "## Extract parameters.\n",
    "beta_mu   = samples.filter(regex='beta_mu\\[').values\n",
    "beta_1_pr = samples.filter(regex='beta_1_pr\\[').values\n",
    "beta      = samples.filter(regex='beta\\[').values\n",
    "alpha_mu   = samples.filter(regex='alpha_mu\\[').values\n",
    "alpha      = samples.filter(regex='alpha\\[').values\n",
    "sigma     = samples.filter(regex='sigma\\[').values\n",
    "c = -0.625\n",
    "\n",
    "## Item difficulty (mean).\n",
    "mu = beta.mean()\n",
    "lb, ub = hdi(beta.mean(axis=1), hdi_prob=0.95)\n",
    "print('beta (mean):\\t%0.3f [%0.3f, %0.3f]' %(mu, lb, ub))\n",
    "\n",
    "## Item difficulty (variance).\n",
    "mu = beta.std(axis=1).mean()\n",
    "lb, ub = hdi(beta.std(axis=1), hdi_prob=0.95)\n",
    "print('beta (var):\\t%0.3f [%0.3f, %0.3f]' %(mu, lb, ub))\n",
    "\n",
    "## Item discrimination (mean).\n",
    "mu = alpha.mean()\n",
    "lb, ub = hdi(alpha.mean(axis=1), hdi_prob=0.95)\n",
    "print('alpha (mean):\\t%0.3f [%0.3f, %0.3f]' %(mu, lb, ub))\n",
    "\n",
    "## Item difficulty (variance).\n",
    "mu = alpha.std(axis=1).mean()\n",
    "lb, ub = hdi(beta.std(axis=1), hdi_prob=0.95)\n",
    "print('alpha (var):\\t%0.3f [%0.3f, %0.3f]' %(mu, lb, ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc18eab-af57-466a-afcc-75e677fc61f5",
   "metadata": {},
   "source": [
    "### 2.3 Interpreting parameters\n",
    "\n",
    "#### Item difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93d3c891-c622-4650-8a86-c9ea12bbfb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_fea:\t0.142 [0.096, 0.188]\n",
      "n_rul:\t0.126 [0.093, 0.162]\n",
      "distr:\t0.266 [0.229, 0.305]\n",
      "rt:\t0.132 [0.100, 0.163]\n"
     ]
    }
   ],
   "source": [
    "## Define useful functions.\n",
    "def inv_logit(x):\n",
    "    return 1. / (1 + np.exp(-x))\n",
    "\n",
    "## Load design.\n",
    "X2 = read_csv(os.path.join('designs', 'X2.csv'))\n",
    "\n",
    "## Iterate over fixed effects.\n",
    "for i, col in enumerate(['n_features', 'n_rules', 'distractor', 'rt']):\n",
    "    \n",
    "    ## Compute regressor SD.\n",
    "    sd = X2[col].std()\n",
    "    \n",
    "    ## Compute change in response.\n",
    "    delta = inv_logit(beta_mu[:,0] + 0.5 * (beta_mu[:,i+1] / sd))  -\\\n",
    "            inv_logit(beta_mu[:,0] - 0.5 * (beta_mu[:,i+1] / sd))\n",
    "    \n",
    "    ## Compute summary stats.\n",
    "    mu = delta.mean()\n",
    "    lb, ub = hdi(delta, hdi_prob=0.95)\n",
    "    \n",
    "    ## Print.\n",
    "    print('%s:\\t%0.3f [%0.3f, %0.3f]' %(col[:5],mu,lb,ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce88a28-5bff-4937-9508-4e690717b5de",
   "metadata": {},
   "source": [
    "#### Item discrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63faeacd-da2e-4aaf-b2d0-954690d64dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_fea:\t-0.037 [-0.090, 0.017]\n",
      "n_rul:\t0.067 [0.011, 0.123]\n",
      "distr:\t-0.036 [-0.082, 0.006]\n",
      "rt:\t-0.025 [-0.075, 0.024]\n"
     ]
    }
   ],
   "source": [
    "## Iterate over fixed effects.\n",
    "for i, col in enumerate(['n_features', 'n_rules', 'distractor', 'rt']):\n",
    "    \n",
    "    ## Compute summary stats.\n",
    "    mu = alpha_mu[:,i+1].mean()\n",
    "    lb, ub = hdi(alpha_mu[:,i+1], hdi_prob=0.95)\n",
    "    \n",
    "    ## Print.\n",
    "    print('%s:\\t%0.3f [%0.3f, %0.3f]' %(col[:5],mu,lb,ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd37240a-74ce-42fa-8a3b-e9a96bfc3b04",
   "metadata": {},
   "source": [
    "### 2.4 Variance explained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0741eace-1840-4c92-ad1c-67ccd761e464",
   "metadata": {},
   "source": [
    "#### Item difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ce3930c-698d-4415-bb3b-09030b2583bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level-1   fixed:\t 0.364\n",
      "all item  fixed:\t 0.491\n",
      "level-1,2 fixed:\t 0.662\n",
      "all effects:    \t 0.841\n"
     ]
    }
   ],
   "source": [
    "## Load and prepare design matrices.\n",
    "zscore = lambda x: (x - np.nanmean(x)) / np.nanstd(x)\n",
    "X2 = read_csv(os.path.join('designs', 'X2.csv')).apply(zscore, 0).fillna(1).values\n",
    "X3 = read_csv(os.path.join('designs', 'X3.csv')).values.astype(float)\n",
    "\n",
    "## Compute total variance.\n",
    "total_var = np.var(beta, axis=1).mean()\n",
    "\n",
    "## Compute explained variance (level-1 fixed effects).\n",
    "var = np.var(beta - beta_mu[:,[1,2]] @ X2.T[[1,2]], axis=1).mean()\n",
    "print('level-1   fixed:\\t %0.3f' %(1 - var / total_var))\n",
    "\n",
    "## Compute explained variance (item fixed effects).\n",
    "var = np.var(beta - beta_mu[:,[1,2,3]] @ X2.T[[1,2,3]], axis=1).mean()\n",
    "print('all item  fixed:\\t %0.3f' %(1 - var / total_var))\n",
    "\n",
    "## Compute explained variance (level-1,2 fixed effects).\n",
    "var = np.var(beta - beta_mu @ X2.T, axis=1).mean()\n",
    "print('level-1,2 fixed:\\t %0.3f' %(1 - var / total_var))\n",
    "\n",
    "## Compute variance explained (all effects).\n",
    "mu = beta_mu @ X2.T + beta_1_pr @ X3.T * np.atleast_2d(sigma[:,0]).T\n",
    "var = np.var(beta - mu, axis=1).mean()\n",
    "print('all effects:    \\t %0.3f' %(1 - var / total_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c1d7e7-bca8-435a-8194-c90772bce9ba",
   "metadata": {},
   "source": [
    "#### Item Discrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e147340-3269-4833-b420-a88ccbadbfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level-1,2 fixed:\t 0.449\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "## Load and prepare design matrices.\n",
    "zscore = lambda x: (x - np.nanmean(x)) / np.nanstd(x)\n",
    "X2 = read_csv(os.path.join('designs', 'X2.csv')).apply(zscore, 0).fillna(1).values\n",
    "\n",
    "## Compute total variance.\n",
    "total_var = np.var(alpha, axis=1).mean()\n",
    "\n",
    "## Variance explained (level-1).\n",
    "mu = norm.cdf(c + alpha_mu @ X2.T) * 5\n",
    "var = np.var(alpha - mu, axis=1).mean()\n",
    "print('level-1,2 fixed:\\t %0.3f' %(1 - var / total_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217fc1bd-83cc-4a56-9afb-94d621216fe9",
   "metadata": {},
   "source": [
    "## Section 3: Explanatory IRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b405590-ae48-447b-be58-087a893e7278",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract parameters.\n",
    "theta = samples.filter(regex='theta\\[').values\n",
    "rho   = samples.filter(regex='rho\\[').values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdc1ccc-04bb-4aa7-99e1-e135bc1a98d0",
   "metadata": {},
   "source": [
    "### 3.1 Interpreting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff5656b4-189d-4909-b9a2-1799f6bbedf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age:\t-0.299 [-0.353, -0.246]\n",
      "gende:\t0.128 [0.078, 0.182]\n",
      "rt_0:\t0.427 [0.374, 0.478]\n",
      "rt_1:\t0.368 [0.313, 0.424]\n"
     ]
    }
   ],
   "source": [
    "## Load and prepare design matrices.\n",
    "X1 = read_csv(os.path.join('designs', 'X1.csv'))\n",
    "\n",
    "for i, col in enumerate(X1.columns):\n",
    "    \n",
    "    ## Compute summary stats.\n",
    "    mu = rho[:,i].mean()\n",
    "    lb, ub = hdi(rho[:,i], hdi_prob=0.95)\n",
    "    \n",
    "    ## Print.\n",
    "    print('%s:\\t%0.3f [%0.3f, %0.3f]' %(col[:5],mu,lb,ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7c5827-8e9e-4894-9fe8-407d6409a8f1",
   "metadata": {},
   "source": [
    "#### 3.2 Variance explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "432a3911-8266-4c9e-b2c8-9d4ad7959309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all effects: \t 0.502\n"
     ]
    }
   ],
   "source": [
    "## Load and prepare design matrices.\n",
    "zscore = lambda x: (x - np.nanmean(x)) / np.nanstd(x)\n",
    "X1 = read_csv(os.path.join('designs', 'X1.csv')).apply(zscore, 0).fillna(1).values\n",
    "\n",
    "## Compute total variance.\n",
    "total_var = np.var(theta, axis=1).mean()\n",
    "\n",
    "## Variance explained (level-1).\n",
    "var = np.var(theta - rho @ X1.T, axis=1).mean()\n",
    "print('all effects: \\t %0.3f' %(1 - var / total_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69a3700-3b74-4e4f-a940-714fb3c551c3",
   "metadata": {},
   "source": [
    "## Section 4: Model Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32faf298-9231-417e-9dce-465809367db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_csv('stan_results/3pl_m2_ppc.csv')\n",
    "sx = read_csv('stan_results/3pl_m2_ifdm.csv', index_col=0)\n",
    "gb = df.groupby('item_id').agg({'accuracy':'mean', 'y_hat':'mean'})\n",
    "gb = gb.merge(sx, left_index=True, right_index=True)\n",
    "gb['thresh'] = np.where(np.logical_or(gb.pval < 0.05, gb.pval > 0.95), 1, 0)\n",
    "ax = sns.scatterplot(x='accuracy', y='y_hat', hue='thresh', data=gb)\n",
    "ax.plot([0,1],[0,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
