{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82f4016f-87bd-41dd-90eb-e92afe20fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from arviz import hdi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcec38db-a532-4a6f-aa3b-b64522e11210",
   "metadata": {},
   "source": [
    "## Section 1: 1PL-G Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d93689-ad90-47f9-8db8-28dcf07af6b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1 Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5e172f1-9741-460a-9123-9d5f8d8ada41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1plg_m1: 27741.4 | 105.4\n",
      "1plg_m2: 26962.1 | 164.0\n",
      "1plg_m3: 26585.5 | 339.5\n"
     ]
    }
   ],
   "source": [
    "for i, m in enumerate(['1plg_m1','1plg_m2','1plg_m3']):\n",
    "    \n",
    "    ## Load ppc.\n",
    "    ppc = read_csv(os.path.join('stan_results', f'{m}_ppc.csv'))\n",
    "        \n",
    "    ## Model comparison.\n",
    "    loco = -2 * ppc.groupby('subject').loco.mean().sum()\n",
    "    pwaic_c = ppc.groupby('subject').pwaic_c.mean().sum()\n",
    "    \n",
    "    print('%s: %0.1f | %0.1f' %(m, loco, pwaic_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1387114-760b-4392-b426-6918aaabdedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1plg_m2 - 1plg_m1: -779.3 (51.4)\n",
      "1plg_m3 - 1plg_m1: -1155.8 (60.1)\n",
      "1plg_m3 - 1plg_m2: -376.5 (34.5)\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "for a, b in list(combinations(['1plg_m1','1plg_m2','1plg_m3'], 2)):\n",
    "    \n",
    "    ## Load data.\n",
    "    df1 = read_csv(os.path.join('stan_results', f'{a}_ppc.csv'))\n",
    "    df2 = read_csv(os.path.join('stan_results', f'{b}_ppc.csv'))\n",
    "    \n",
    "    arr = -2 * (df2.groupby('subject').loco.mean() - df1.groupby('subject').loco.mean())\n",
    "    \n",
    "    ## Compute stats.\n",
    "    N = df1.subject.nunique()\n",
    "    mu = np.sum(arr)\n",
    "    se = np.std(arr) * np.sqrt(N)\n",
    "    \n",
    "    print(f'{b} - {a}: %0.1f (%0.1f)' %(mu, se))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc10e2d7-1b34-4e8b-a411-eea581f7e434",
   "metadata": {},
   "source": [
    "### 1.2 Parameter distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "889308e6-f595-4d20-9056-7fff56d349c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta (mean):\t0.177 [0.116, 0.236]\n",
      "beta (var):\t1.431 [1.368, 1.493]\n"
     ]
    }
   ],
   "source": [
    "## Load samples.\n",
    "samples = read_csv(os.path.join('stan_results', '1plg_m3.tsv.gz'), sep='\\t', compression='gzip')\n",
    "\n",
    "## Extract parameters.\n",
    "beta_mu   = samples.filter(regex='beta_mu\\[').values\n",
    "beta_1_pr = samples.filter(regex='beta_1_pr\\[').values\n",
    "beta      = samples.filter(regex='beta\\[').values\n",
    "sigma     = samples.filter(regex='sigma\\[').values\n",
    "\n",
    "## Item difficulty (mean).\n",
    "mu = beta.mean()\n",
    "lb, ub = hdi(beta.mean(axis=1), hdi_prob=0.95)\n",
    "print('beta (mean):\\t%0.3f [%0.3f, %0.3f]' %(mu, lb, ub))\n",
    "\n",
    "## Item difficulty (variance).\n",
    "mu = beta.std(axis=1).mean()\n",
    "lb, ub = hdi(beta.std(axis=1), hdi_prob=0.95)\n",
    "print('beta (var):\\t%0.3f [%0.3f, %0.3f]' %(mu, lb, ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43546293-f0a5-4a99-aa27-d02885832481",
   "metadata": {},
   "source": [
    "### 1.3 Interpreting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b4d8f3a-fecd-4b29-ab27-0ce19b02211d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_fea:\t0.132 [0.089, 0.176]\n",
      "n_rul:\t0.112 [0.079, 0.143]\n",
      "distr:\t0.252 [0.218, 0.287]\n",
      "rt:\t0.122 [0.093, 0.151]\n"
     ]
    }
   ],
   "source": [
    "## Define useful functions.\n",
    "def inv_logit(x):\n",
    "    return 1. / (1 + np.exp(-x))\n",
    "\n",
    "## Load design.\n",
    "X2 = read_csv(os.path.join('designs', 'X2.csv'))\n",
    "\n",
    "## Iterate over fixed effects.\n",
    "for i, col in enumerate(['n_features', 'n_rules', 'distractor', 'rt']):\n",
    "    \n",
    "    ## Compute regressor SD.\n",
    "    sd = X2[col].std()\n",
    "    \n",
    "    ## Compute change in response.\n",
    "    delta = inv_logit(beta_mu[:,0] + 0.5 * (beta_mu[:,i+1] / sd))  -\\\n",
    "            inv_logit(beta_mu[:,0] - 0.5 * (beta_mu[:,i+1] / sd))\n",
    "    \n",
    "    ## Compute summary stats.\n",
    "    mu = delta.mean()\n",
    "    lb, ub = hdi(delta, hdi_prob=0.95)\n",
    "    \n",
    "    ## Print.\n",
    "    print('%s:\\t%0.3f [%0.3f, %0.3f]' %(col[:5],mu,lb,ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba7750-7afa-47c5-903a-58cb7447d5fe",
   "metadata": {},
   "source": [
    "### 1.4 Variance explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7be023f-149e-42ea-8cc0-5da6b7761781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:\t0.641\n",
      "n_feat:\t0.166\n",
      "n_rule:\t0.209\n",
      "lvl-1:\t0.373\n"
     ]
    }
   ],
   "source": [
    "## Load and prepare design matrices.\n",
    "zscore = lambda x: (x - np.nanmean(x)) / np.nanstd(x)\n",
    "X2 = read_csv(os.path.join('designs', 'X2.csv')).apply(zscore, 0).fillna(1).values\n",
    "X3 = read_csv(os.path.join('designs', 'X3.csv')).values.astype(float)\n",
    "\n",
    "## Compute variance explained by fixed effects (total).\n",
    "beta_hat = X2 @ beta_mu.T\n",
    "error = beta.T - beta_hat\n",
    "r2 = np.var(beta_hat, axis=0).mean() / (np.var(beta_hat, axis=0).mean() + np.var(error, axis=0).mean())\n",
    "print('total:\\t%0.3f' %r2)\n",
    "\n",
    "for ix, label in zip([[0,2,3,4],[0,1,3,4],[0,3,4]], ['n_feat','n_rule','lvl-1']):\n",
    "    \n",
    "    ## Compute predicted / error terms.\n",
    "    beta_hat = X2[:,ix] @ beta_mu.T[ix]\n",
    "    error = beta.T - beta_hat\n",
    "    \n",
    "    ## Compute variance explained.\n",
    "    a = np.var(beta_hat, axis=0).mean()\n",
    "    b = np.var(error, axis=0).mean()\n",
    "    \n",
    "    r2p = a / (a + b)\n",
    "    print('%s:\\t%0.3f' %(label, r2-r2p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a8da1b7-6888-442d-ac5e-8db69308e316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lvl-1:\t0.661\n"
     ]
    }
   ],
   "source": [
    "## Compute level-1 coefficients.\n",
    "beta_0 = beta_mu[:,:3] @ X2.T[:3]\n",
    "beta_1 = beta_0 + beta_1_pr @ X3.T * np.atleast_2d(sigma[:,0]).T\n",
    "error = beta_1 - beta_0\n",
    "\n",
    "## Compute variance explained.\n",
    "a = np.var(beta_0, axis=1).mean()\n",
    "b = np.var(error, axis=1).mean()\n",
    "r2 = a / (a + b)\n",
    "\n",
    "print('lvl-1:\\t%0.3f' %r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2654d597-de26-4fa1-b473-9316456d286e",
   "metadata": {},
   "source": [
    "## Section 2: 3PL Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6f1a9c-943a-461b-9f52-f6cd738a0c33",
   "metadata": {},
   "source": [
    "### 2.1 Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87036b8e-7f68-4a8a-a51f-fb13038d77fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3pl_m1: 26419.6 | 385.47\n",
      "3pl_m2: 26414.5 | 408.13\n",
      "3pl_m3: 26417.8 | 428.17\n"
     ]
    }
   ],
   "source": [
    "for i, m in enumerate(['3pl_m1','3pl_m2','3pl_m3']):\n",
    "    \n",
    "    ## Load ppc.\n",
    "    ppc = read_csv(os.path.join('stan_results', f'{m}_ppc.csv'))\n",
    "        \n",
    "    ## Model comparison.\n",
    "    loco = -2 * ppc.groupby('subject').loco.mean().sum()\n",
    "    pwaic_c = ppc.groupby('subject').pwaic_c.mean().sum()\n",
    "    \n",
    "    print('%s: %0.1f | %0.2f' %(m, loco, pwaic_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbff71aa-c119-41cc-9ebd-05529d82f4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3pl_m2 - 3pl_m1: -5.12 (4.07)\n",
      "3pl_m3 - 3pl_m1: -1.83 (4.35)\n",
      "3pl_m3 - 3pl_m2: 3.29 (1.84)\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "for a, b in list(combinations(['3pl_m1','3pl_m2','3pl_m3'], 2)):\n",
    "    \n",
    "    ## Load data.\n",
    "    df1 = read_csv(os.path.join('stan_results', f'{a}_ppc.csv'))\n",
    "    df2 = read_csv(os.path.join('stan_results', f'{b}_ppc.csv'))\n",
    "    \n",
    "    arr = -2 * (df2.groupby('subject').loco.mean() - df1.groupby('subject').loco.mean())\n",
    "    \n",
    "    ## Compute stats.\n",
    "    N = df1.subject.nunique()\n",
    "    mu = np.sum(arr)\n",
    "    se = np.std(arr) * np.sqrt(N)\n",
    "    \n",
    "    print(f'{b} - {a}: %0.2f (%0.2f)' %(mu, se))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa5a4138-fe1b-4349-b534-38cb59c4c4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3pl_m2 - 1plg_m3: 85.53 (6.32)\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "for a, b in list(combinations(['1plg_m3','3pl_m2'], 2)):\n",
    "    \n",
    "    ## Load data.\n",
    "    df1 = read_csv(os.path.join('stan_results', f'{a}_ppc.csv'))\n",
    "    df2 = read_csv(os.path.join('stan_results', f'{b}_ppc.csv'))\n",
    "    \n",
    "    arr = df2.groupby('subject').loco.mean() - df1.groupby('subject').loco.mean()\n",
    "    \n",
    "    ## Compute stats.\n",
    "    N = df1.subject.nunique()\n",
    "    mu = np.sum(arr)\n",
    "    se = np.std(arr) * np.sqrt(N)\n",
    "    \n",
    "    print(f'{b} - {a}: %0.2f (%0.2f)' %(mu, se))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5c0ceb-777f-4f85-86a8-8df1d0d1c797",
   "metadata": {},
   "source": [
    "### 2.2 Parameter distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "625591a0-cfd4-4cef-9c8b-12fa8181481b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta (mean):\t0.221 [0.142, 0.302]\n",
      "beta (var):\t1.554 [1.475, 1.629]\n",
      "beta (min):\t-3.704 [-4.687, -2.775]\n",
      "beta (max):\t3.497 [2.481, 4.635]\n",
      "alpha (mean):\t1.298 [1.212, 1.385]\n",
      "alpha (var):\t0.221 [0.121, 0.322]\n",
      "rho:\t\t-0.060 [-0.379, 0.250]\n"
     ]
    }
   ],
   "source": [
    "## Load samples.\n",
    "samples = read_csv(os.path.join('stan_results', '3pl_m2.tsv.gz'), sep='\\t', compression='gzip')\n",
    "\n",
    "## Extract parameters.\n",
    "beta_mu   = samples.filter(regex='beta_mu\\[').values\n",
    "beta_1_pr = samples.filter(regex='beta_1_pr\\[').values\n",
    "beta      = samples.filter(regex='beta\\[').values\n",
    "alpha_mu   = samples.filter(regex='alpha_mu\\[').values\n",
    "alpha      = samples.filter(regex='alpha\\[').values\n",
    "sigma     = samples.filter(regex='sigma\\[').values\n",
    "c = -0.625\n",
    "\n",
    "## Item difficulty (mean).\n",
    "mu = beta.mean()\n",
    "lb, ub = hdi(beta.mean(axis=1), hdi_prob=0.95)\n",
    "print('beta (mean):\\t%0.3f [%0.3f, %0.3f]' %(mu, lb, ub))\n",
    "\n",
    "## Item difficulty (variance).\n",
    "mu = beta.std(axis=1).mean()\n",
    "lb, ub = hdi(beta.std(axis=1), hdi_prob=0.95)\n",
    "print('beta (var):\\t%0.3f [%0.3f, %0.3f]' %(mu, lb, ub))\n",
    "\n",
    "## Item difficulty (min).\n",
    "arr = beta[:,np.argmin(beta.mean(axis=0))]\n",
    "mu = arr.mean()\n",
    "lb, ub = hdi(arr, hdi_prob=0.95)\n",
    "print('beta (min):\\t%0.3f [%0.3f, %0.3f]' %(mu, lb, ub))\n",
    "\n",
    "## Item difficulty (max).\n",
    "arr = beta[:,np.argmax(beta.mean(axis=0))]\n",
    "mu = arr.mean()\n",
    "lb, ub = hdi(arr, hdi_prob=0.95)\n",
    "print('beta (max):\\t%0.3f [%0.3f, %0.3f]' %(mu, lb, ub))\n",
    "\n",
    "## Item discrimination (mean).\n",
    "mu = alpha.mean()\n",
    "lb, ub = hdi(alpha.mean(axis=1), hdi_prob=0.95)\n",
    "print('alpha (mean):\\t%0.3f [%0.3f, %0.3f]' %(mu, lb, ub))\n",
    "\n",
    "## Item difficulty (variance).\n",
    "mu = alpha.std(axis=1).mean()\n",
    "lb, ub = hdi(alpha.std(axis=1), hdi_prob=0.95)\n",
    "print('alpha (var):\\t%0.3f [%0.3f, %0.3f]' %(mu, lb, ub))\n",
    "\n",
    "## Compute correlation.\n",
    "rho = np.array([np.corrcoef(a,b)[0,1] for a, b in zip(alpha, beta)])\n",
    "mu = rho.mean()\n",
    "lb, ub = hdi(rho, hdi_prob=0.95)\n",
    "print('rho:\\t\\t%0.3f [%0.3f, %0.3f]' %(mu, lb, ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc18eab-af57-466a-afcc-75e677fc61f5",
   "metadata": {},
   "source": [
    "### 2.3 Interpreting parameters\n",
    "\n",
    "#### Item difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93d3c891-c622-4650-8a86-c9ea12bbfb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_fea:\t0.579 [0.389, 0.770]\n",
      "n_rul:\t0.514 [0.378, 0.663]\n",
      "distr:\t1.105 [0.940, 1.269]\n",
      "rt:\t0.541 [0.414, 0.671]\n"
     ]
    }
   ],
   "source": [
    "## Define useful functions.\n",
    "def inv_logit(x):\n",
    "    return 1. / (1 + np.exp(-x))\n",
    "\n",
    "## Load design.\n",
    "X2 = read_csv(os.path.join('designs', 'X2.csv'))\n",
    "\n",
    "## Iterate over fixed effects.\n",
    "for i, col in enumerate(['n_features', 'n_rules', 'distractor', 'rt']):\n",
    "    \n",
    "    ## Compute regressor SD.\n",
    "    sd = X2[col].std()\n",
    "    \n",
    "    ## Compute change in response.\n",
    "    arr = beta_mu[:,i+1] / sd\n",
    "    \n",
    "    ## Compute summary stats.\n",
    "    mu = arr.mean()\n",
    "    lb, ub = hdi(arr, hdi_prob=0.95)\n",
    "    \n",
    "    ## Print.\n",
    "    print('%s:\\t%0.3f [%0.3f, %0.3f]' %(col[:5],mu,lb,ub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d87c40b-5808-4f92-bed1-ebed85278960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_fea:\t0.106 [0.072, 0.141]\n",
      "n_rul:\t0.095 [0.070, 0.122]\n",
      "distr:\t0.199 [0.172, 0.228]\n",
      "rt:\t0.099 [0.075, 0.122]\n"
     ]
    }
   ],
   "source": [
    "## Define useful functions.\n",
    "def inv_logit(x):\n",
    "    return 1. / (1 + np.exp(-x))\n",
    "\n",
    "## Load design.\n",
    "X2 = read_csv(os.path.join('designs', 'X2.csv'))\n",
    "\n",
    "## Iterate over fixed effects.\n",
    "for i, col in enumerate(['n_features', 'n_rules', 'distractor', 'rt']):\n",
    "    \n",
    "    ## Compute regressor SD.\n",
    "    sd = X2[col].std()\n",
    "    \n",
    "    ## Compute change in response.\n",
    "    delta = (0.25 + (1-0.25) * inv_logit(beta_mu[:,0] + 0.5 * (beta_mu[:,i+1] / sd)))  -\\\n",
    "            (0.25 + (1-0.25) * inv_logit(beta_mu[:,0] - 0.5 * (beta_mu[:,i+1] / sd)))\n",
    "    \n",
    "    ## Compute summary stats.\n",
    "    mu = delta.mean()\n",
    "    lb, ub = hdi(delta, hdi_prob=0.95)\n",
    "    \n",
    "    ## Print.\n",
    "    print('%s:\\t%0.3f [%0.3f, %0.3f]' %(col[:5],mu,lb,ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce88a28-5bff-4937-9508-4e690717b5de",
   "metadata": {},
   "source": [
    "#### Item discrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63faeacd-da2e-4aaf-b2d0-954690d64dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_fea:\t-0.037 [-0.090, 0.017]\n",
      "n_rul:\t0.067 [0.011, 0.123]\n",
      "distr:\t-0.036 [-0.082, 0.006]\n",
      "rt:\t-0.025 [-0.075, 0.024]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "## Iterate over fixed effects.\n",
    "for i, col in enumerate(['n_features', 'n_rules', 'distractor', 'rt']):\n",
    "    \n",
    "    ## Compute summary stats.\n",
    "    mu = alpha_mu[:,i+1].mean()\n",
    "    lb, ub = hdi(alpha_mu[:,i+1], hdi_prob=0.95)\n",
    "    \n",
    "    ## Print.\n",
    "    print('%s:\\t%0.3f [%0.3f, %0.3f]' %(col[:5],mu,lb,ub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18c944b6-9cde-43d2-b886-abfce6d2c2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_fea:\t-0.015 [-0.036, 0.008]\n",
      "n_rul:\t0.020 [0.003, 0.037]\n",
      "distr:\t-0.029 [-0.065, 0.005]\n",
      "rt:\t-0.010 [-0.030, 0.009]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "## Load design.\n",
    "X2 = read_csv(os.path.join('designs', 'X2.csv'))\n",
    "\n",
    "## Iterate over fixed effects.\n",
    "for i, col in enumerate(['n_features', 'n_rules', 'distractor', 'rt']):\n",
    "    \n",
    "    ## Compute regressor SD.\n",
    "    sd = X2[col].std()\n",
    "    \n",
    "    ## Compute change in discrimination.\n",
    "    delta = norm.cdf(alpha_mu[:,0] + 0.5 * (alpha_mu[:,i+1] / sd))  -\\\n",
    "            norm.cdf(alpha_mu[:,0] - 0.5 * (alpha_mu[:,i+1] / sd))\n",
    "    \n",
    "    ## Compute summary stats.\n",
    "    mu = delta.mean()\n",
    "    lb, ub = hdi(delta, hdi_prob=0.95)\n",
    "    \n",
    "    ## Print.\n",
    "    print('%s:\\t%0.3f [%0.3f, %0.3f]' %(col[:5],mu,lb,ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd37240a-74ce-42fa-8a3b-e9a96bfc3b04",
   "metadata": {},
   "source": [
    "### 2.4 Variance explained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0741eace-1840-4c92-ad1c-67ccd761e464",
   "metadata": {},
   "source": [
    "#### Item difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ce3930c-698d-4415-bb3b-09030b2583bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:\t0.649\n",
      "n_feat:\t0.163\n",
      "n_rule:\t0.225\n",
      "lvl-1:\t0.386\n",
      "big 3:\t0.520\n"
     ]
    }
   ],
   "source": [
    "## Load and prepare design matrices.\n",
    "zscore = lambda x: (x - np.nanmean(x)) / np.nanstd(x)\n",
    "X2 = read_csv(os.path.join('designs', 'X2.csv')).apply(zscore, 0).fillna(1).values\n",
    "X3 = read_csv(os.path.join('designs', 'X3.csv')).values.astype(float)\n",
    "\n",
    "## Compute variance explained by fixed effects (total).\n",
    "beta_hat = X2 @ beta_mu.T\n",
    "error = beta.T - beta_hat\n",
    "r2 = np.var(beta_hat, axis=0).mean() / (np.var(beta_hat, axis=0).mean() + np.var(error, axis=0).mean())\n",
    "print('total:\\t%0.3f' %r2)\n",
    "\n",
    "for ix, label in zip([[0,2,3,4],[0,1,3,4],[0,3,4],[0,4]], ['n_feat','n_rule','lvl-1','big 3']):\n",
    "    \n",
    "    ## Compute predicted / error terms.\n",
    "    beta_hat = X2[:,ix] @ beta_mu.T[ix]\n",
    "    error = beta.T - beta_hat\n",
    "    \n",
    "    ## Compute variance explained.\n",
    "    a = np.var(beta_hat, axis=0).mean()\n",
    "    b = np.var(error, axis=0).mean()\n",
    "    \n",
    "    r2p = a / (a + b)\n",
    "    print('%s:\\t%0.3f' %(label, r2-r2p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "785c9b83-eb9a-4616-80b3-d6d05517eba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lvl-1:\t0.676\n",
      "n_feat:\t0.295\n",
      "n_rule:\t0.406\n"
     ]
    }
   ],
   "source": [
    "## Compute level-1 coefficients.\n",
    "beta_0 = beta_mu[:,:3] @ X2.T[:3]\n",
    "beta_1 = beta_0 + beta_1_pr @ X3.T * np.atleast_2d(sigma[:,0]).T\n",
    "error = beta_1 - beta_0\n",
    "\n",
    "## Compute variance explained.\n",
    "a = np.var(beta_0, axis=1).mean()\n",
    "b = np.var(error, axis=1).mean()\n",
    "r2 = a / (a + b)\n",
    "print('lvl-1:\\t%0.3f' %r2)\n",
    "\n",
    "for ix, label in zip([[0,2],[0,1]], ['n_feat','n_rule']):\n",
    "    \n",
    "    ## Compute predicted / error terms.\n",
    "    beta_hat = X2[:,ix] @ beta_mu.T[ix]\n",
    "    error = beta_1.T - beta_hat\n",
    "    \n",
    "    ## Compute variance explained.\n",
    "    a = np.var(beta_hat, axis=0).mean()\n",
    "    b = np.var(error, axis=0).mean()\n",
    "    \n",
    "    r2p = a / (a + b)\n",
    "    print('%s:\\t%0.3f' %(label, r2-r2p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c1d7e7-bca8-435a-8194-c90772bce9ba",
   "metadata": {},
   "source": [
    "#### Item Discrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e147340-3269-4833-b420-a88ccbadbfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:\t0.474\n",
      "lvl-1:\t0.319\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "## Load and prepare design matrices.\n",
    "zscore = lambda x: (x - np.nanmean(x)) / np.nanstd(x)\n",
    "X2 = read_csv(os.path.join('designs', 'X2.csv')).apply(zscore, 0).fillna(1).values\n",
    "\n",
    "## Compute predicted / error terms.\n",
    "mu = norm.cdf(c + X2 @ alpha_mu.T) * 5\n",
    "error = alpha.T - mu\n",
    "\n",
    "## Compute variance explained.\n",
    "a = np.var(mu, axis=0).mean()\n",
    "b = np.var(error, axis=0).mean()\n",
    "r2 = a / (a + b)\n",
    "print('total:\\t%0.3f' %r2)\n",
    "\n",
    "for ix, label in zip([[0,3,4]], ['lvl-1']):\n",
    "    \n",
    "    ## Compute predicted / error terms.\n",
    "    mu = norm.cdf(c + X2[:,ix] @ alpha_mu.T[ix]) * 5\n",
    "    error = alpha.T - mu\n",
    "    \n",
    "    ## Compute variance explained.\n",
    "    a = np.var(mu, axis=0).mean()\n",
    "    b = np.var(error, axis=0).mean()\n",
    "    \n",
    "    r2p = a / (a + b)\n",
    "    print('%s:\\t%0.3f' %(label, r2-r2p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7ecd88-8684-4b6a-bac1-db8516fb1977",
   "metadata": {},
   "source": [
    "### 2.5 Contrasts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e03963-d40d-4d1b-b8c4-fcd49de7d4af",
   "metadata": {},
   "source": [
    "#### Item diffculty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e541f1fc-e22a-4a20-a243-a0d8ff703f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ss2 - ss1: 0.149 [0.054, 0.241]\n",
      "ss3 - ss1: 0.310 [0.213, 0.413]\n",
      "ss3 - ss2: 0.161 [0.066, 0.259]\n"
     ]
    }
   ],
   "source": [
    "## Load design data.\n",
    "design = read_csv(os.path.join('designs', 'features.csv'), usecols=np.arange(6))\n",
    "\n",
    "for a, b in list(combinations([1,2,3], 2)):\n",
    "    \n",
    "    ## Compute average paired differences of item difficulty.\n",
    "    delta = np.mean(beta[:,design.shape_set == b] -  beta[:,design.shape_set == a], axis=1)\n",
    "    \n",
    "    ## Compute contrast.\n",
    "    mu = np.mean(delta)\n",
    "    lb, ub = hdi(delta, hdi_prob=0.95)\n",
    "    \n",
    "    ## Print info.\n",
    "    print('ss%s - ss%s: %0.3f [%0.3f, %0.3f]' %(b, a, mu, lb, ub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "188ea4b5-3a3e-4b94-8bb9-a528c7d1404a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma[2]: 0.620 [0.538, 0.701]\n"
     ]
    }
   ],
   "source": [
    "mu = np.median(sigma[:,1])\n",
    "lb, ub = hdi(sigma[:,1], hdi_prob=0.95)\n",
    "\n",
    "print('sigma[2]: %0.3f [%0.3f, %0.3f]' %(mu, lb, ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217fc1bd-83cc-4a56-9afb-94d621216fe9",
   "metadata": {},
   "source": [
    "## Section 3: Explanatory IRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b405590-ae48-447b-be58-087a893e7278",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract parameters.\n",
    "theta = samples.filter(regex='theta\\[').values\n",
    "rho   = samples.filter(regex='rho\\[').values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdc1ccc-04bb-4aa7-99e1-e135bc1a98d0",
   "metadata": {},
   "source": [
    "### 3.1 Interpreting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff5656b4-189d-4909-b9a2-1799f6bbedf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age:\t-0.299 [-0.353, -0.246]\n",
      "gende:\t0.128 [0.078, 0.182]\n",
      "rt_0:\t0.427 [0.374, 0.478]\n",
      "rt_1:\t0.368 [0.313, 0.424]\n"
     ]
    }
   ],
   "source": [
    "## Load and prepare design matrices.\n",
    "X1 = read_csv(os.path.join('designs', 'X1.csv'))\n",
    "\n",
    "for i, col in enumerate(X1.columns):\n",
    "    \n",
    "    ## Compute summary stats.\n",
    "    mu = rho[:,i].mean()\n",
    "    lb, ub = hdi(rho[:,i], hdi_prob=0.95)\n",
    "    \n",
    "    ## Print.\n",
    "    print('%s:\\t%0.3f [%0.3f, %0.3f]' %(col[:5],mu,lb,ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7c5827-8e9e-4894-9fe8-407d6409a8f1",
   "metadata": {},
   "source": [
    "#### 3.2 Variance explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "432a3911-8266-4c9e-b2c8-9d4ad7959309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:\t0.503\n"
     ]
    }
   ],
   "source": [
    "## Load and prepare design matrices.\n",
    "zscore = lambda x: (x - np.nanmean(x)) / np.nanstd(x)\n",
    "X1 = read_csv(os.path.join('designs', 'X1.csv')).apply(zscore, 0).fillna(1).values\n",
    "\n",
    "## Compute predicted / error terms.\n",
    "theta_mu = X1 @ rho.T\n",
    "error = theta.T - theta_mu\n",
    "\n",
    "## Compute variance explained.\n",
    "a = np.var(theta_mu, axis=0).mean()\n",
    "b = np.var(error, axis=0).mean()\n",
    "r2 = a / (a + b)\n",
    "print('total:\\t%0.3f' %r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69a3700-3b74-4e4f-a940-714fb3c551c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 4: Model Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32faf298-9231-417e-9dce-465809367db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_csv('stan_results/3pl_m2_ppc.csv')\n",
    "sx = read_csv('stan_results/3pl_m2_ifdm.csv', index_col=0)\n",
    "gb = df.groupby('item_id').agg({'accuracy':'mean', 'y_hat':'mean'})\n",
    "gb = gb.merge(sx, left_index=True, right_index=True)\n",
    "gb['thresh'] = np.where(np.logical_or(gb.pval < 0.05, gb.pval > 0.95), 1, 0)\n",
    "ax = sns.scatterplot(x='accuracy', y='y_hat', hue='thresh', data=gb)\n",
    "ax.plot([0,1],[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc6332f-c834-4dd6-9e2a-64f0169dda1e",
   "metadata": {},
   "source": [
    "## Section 5: Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c12fab-1676-49d7-b2a3-43d52166d7b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.1 Figure 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab51e84-2f17-4e74-aaee-0a26d957cb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme('notebook', style='white', font='sans-serif', font_scale=1.33)\n",
    "\n",
    "## Load and prepare data.\n",
    "summary = read_csv(os.path.join('stan_results', '3pl_m2_summary.tsv'), sep='\\t', index_col=0)    \n",
    "design = read_csv(os.path.join('designs', 'features.csv'), usecols=np.arange(6))\n",
    "design['beta'] = df.T.filter(regex='beta\\[').T['Mean'].values\n",
    "\n",
    "## Format data.\n",
    "design = design.replace({'md':'MD', 'pd':'PD'})\n",
    "\n",
    "## Initialize canvas.\n",
    "fig, axes = plt.subplots(1, 2, figsize=(9,9), sharex=True, sharey=True)\n",
    "order = design.groupby('item').beta.mean().sort_values().index\n",
    "palette = ['#40adb7', '#5E5A57', '#e6a14c']\n",
    "\n",
    "## Iteratively plot.\n",
    "for ax, hue, label in zip(axes, ['distractor', 'shape_set'], ['A','B']):\n",
    "\n",
    "    ## Plot item difficulty parameters.\n",
    "    sns.stripplot(x='beta', y='item', hue=hue, data=design, order=order, palette=palette,\n",
    "                  orient='h', ax=ax)\n",
    "    \n",
    "    ## Add grid.\n",
    "    ax.grid(axis='y', alpha=0.4)\n",
    "    \n",
    "    ## Add details.\n",
    "    ax.set_yticklabels([str(l) if not i % 2 else '' for i, l in enumerate(order)], fontsize=12)\n",
    "    ax.set(xlim=(-4,4), xlabel='Item Difficulty', ylabel='')\n",
    "    if label == 'A': ax.set_ylabel('Item', fontsize=16)\n",
    "    \n",
    "    ## Add legend.\n",
    "    ax.legend(loc=1, borderaxespad=0, title=hue.replace('_',' ').capitalize(), framealpha=0.95)\n",
    "    \n",
    "    ## Add label.\n",
    "    ax.annotate(label, (0,0), (0,1.005), 'axes fraction', ha='left', va='bottom', \n",
    "                fontsize=24, fontweight='bold')\n",
    "\n",
    "## Adjust figure spacing.\n",
    "sns.despine(left=True, right=True, top=True)\n",
    "plt.subplots_adjust(left=0.08, right=0.98, top=0.95, bottom=0.07, wspace=0.125)\n",
    "\n",
    "## Save figure.\n",
    "plt.savefig(os.path.join('..', '05_Figures', 'fig02.png'), dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1559a85f-28a3-483e-a8b3-f6430efa3bcc",
   "metadata": {},
   "source": [
    "## Section 6: Supplementary materials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4c6b50-504f-4832-a7dc-13d9b13ae6b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6.1 1PL models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76c8a09-22ed-453b-8417-fcdf98d9610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, m in enumerate(['1pl_m1','1pl_m2','1pl_m3']):\n",
    "    \n",
    "    ## Load ppc.\n",
    "    ppc = read_csv(os.path.join('stan_results', f'{m}_ppc.csv'))\n",
    "        \n",
    "    ## Model comparison.\n",
    "    loco = -2 * ppc.groupby('subject').loco.mean().sum()\n",
    "    pwaic_c = ppc.groupby('subject').pwaic_c.mean().sum()\n",
    "    \n",
    "    print('%s: %0.1f | %0.1f' %(m, loco, pwaic_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cce987d-89b8-4e20-9b7d-983f95fbce67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "for m in [1,2,3]:\n",
    "    \n",
    "    ## Load data.\n",
    "    df1 = read_csv(os.path.join('stan_results', f'1plg_m{m}_ppc.csv'))\n",
    "    df2 = read_csv(os.path.join('stan_results', f'1pl_m{m}_ppc.csv'))\n",
    "    \n",
    "    arr = -2 * (df2.groupby('subject').loco.mean() - df1.groupby('subject').loco.mean())\n",
    "    \n",
    "    ## Compute stats.\n",
    "    N = df1.subject.nunique()\n",
    "    mu = np.sum(arr)\n",
    "    se = np.std(arr) * np.sqrt(N)\n",
    "    \n",
    "    print(f'1plg_m{m} - 1pl_m{m}: %0.1f (%0.1f)' %(mu, se))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
