\documentclass[a4paper,man,natbib]{apa6}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{xurl}
\usepackage[colorinlistoftodos]{todonotes}


% specify link style
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=blue
}

% override apa6 section headers
% https://tex.stackexchange.com/questions/125537/how-to-modify-subsubsection-header-apa6-cls
\makeatletter
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}
  {\z@}%
  {\b@level@two@skip}{\e@level@two@skip}%
  {\normalfont\normalsize\bfseries}}
\makeatother

% override apa6 first paragraph index
% https://tex.stackexchange.com/questions/155028/indentation-apa6-class-for-2nd-paragraphs-after-title
% \makeatletter
%   \b@level@one@skip=-2.5ex plus -1ex minus -.2ex
%   \b@level@two@skip=-2.5ex plus -1ex minus -.2ex
% \makeatother

\title{An item response theory analysis of the Matrix Reasoning Item Bank (MaRs-IB)}
\shorttitle{IRT analysis of MaRs-IB}
\author{Samuel Zorowitz$^1$, Gabriele Chierchia$^3$, Sarah-Jayne Blakemore$^3$, Nathaniel D. Daw$^{1,2}$}
\affiliation{$^1$Princeton Neuroscience Institute, Princeton University, USA\\$^2$Department of Psychology, Princeton University, USA\\$^3$Department of Psychology, University of Cambridge, Downing Street, Cambridge, UK}

\abstract{Matrix reasoning tasks are among the most widely used measures of cognitive ability in the behavioral sciences, but the paucity of matrix reasoning tests in the public domain complicates their use. Here we present an extensive investigation and psychometric validation of the matrix reasoning item bank (MaRs-IB), an open-access set of matrix reasoning items. In a first study, we calibrate the psychometric functioning of the items in the MaRs-IB in a large sample adults participants (N=1501). Using additive multilevel item structure models, we establish that the MaRs-IB has many desirable psychometric properties: its items span a wide range of difficulty, possess medium- to-large levels of discrimination, and exhibit as intended certain associations between item attributes and functioning (e.g. item complexity predicts item difficulty). However, we also find that item clones are not psychometrically equivalent and therefore not exchangeable. In a second study, we demonstrate how researchers can use the item parameters we estimated to design new MaRs-IB tests using optimal item assembly. With these methods, we design three parallel short-form measures that exhibited good score reliability and convergent validity in a second sample of adults (N=300). We hope that the materials and results made available here will encourage researchers to use the MaRs-IB in their studies.}

\begin{document}
\maketitle

\section{Introduction}

Matrix reasoning tasks are among the most widely used measures of cognitive ability in the behavioral sciences. Much of their popularity undoubtedly reflects their versatility of use. Matrix reasoning tasks are strong (albeit impure) indicators of general intelligence \citep{gignac2015raven} and working memory capacity \citep{kane2004generality, unsworth2005working}. They are predictive of important real-world outcomes such as childhood academic achievement \citep{roth2015intelligence} and performance on college entrance exams \citep{frey2004scholastic, koenig2008act}. In low-stakes testing settings (i.e. where a participant incurs little or no cost for poor performance), matrix reasoning tasks can additionally function as measures of motivation, willingness to expend mental effort, and other facets of personality \citep{duckworth2011role, gignac2019maximum}. In studies of psychiatric populations, performance on matrix reasoning tasks have also been used to control for general disruptions to cognitive ability when specific domains of cognition are of primary interest \citep{gillan2016characterizing, rouault2018psychiatric, moutoussis2021decision}.

Unfortunately there are a number of obstacles to using matrix reasoning tasks as part of behavioral research. One challenge is the problem of copyright. Many of the most prominent matrix reasoning tasks, such as the WAIS and WASI matrix reasoning subtests \citep{wechsler1999wechsler, corporation2008wechsler}, are not free to use and have legal restrictions against being computerized. A second challenge is that, across all of the matrix reasoning tests in the public domain, there are relatively few unique items available. The Hagen matrices test \citep{heydasch2014hagen} and ICAR matrix reasoning test \citep{condon2014international}, for example, have only 20 and 16 items respectively. The availability of only a limited number of items raises the possibility of repeat exposure effects, which threaten the validity of these measures  \citep{ng1974applicability, bors2003effect}. This problem is exacerbated in the current era of online experiments, where multiple groups of researchers may be inadvertently administering the same test to the same participants recruited from the same online labor platforms.

Noting these challenges, \cite{knoll2016window} / \cite{chierchia2019matrix} developed and made publicly available the matrix reasoning item bank (MaRs-IB), a collection of 80 unique matrix reasoning puzzles. Each puzzle in the MaRs-IB consists of a 3x3 matrix containing abstract shapes in eight out of nine cells (Figure \ref{fig:fig00}). Across cells, some or all of these shapes change according to various arbitrary abstract rules (e.g. color change, position change). To solve a puzzle, participants deduce which of four response options correctly completes the matrix based on these rules. The puzzles in the MaRs-IB vary in their complexity, both in the number of elements per cell and number of rules determining the relations of elements across cells (Figure \ref{fig:fig00}a/b). To increase the reusability of the MaRs-IB, each puzzle (hereafter referred to as an item template) has six clones. A single item's clones share identical structural characteristics (i.e. number of elements, number of rules) but may vary in their distractors (two unique sets per template) or perceptual features (three unique shape sets per template; Figure \ref{fig:fig00}c/d). Thus, the MaRs-IB addresses several of the issues associated with other matrix reasoning tests.   

\cite{chierchia2019matrix} conducted an initial study of the psychometric properties of the MaRs-IB. A sample of 659 adult, adolescent, and child participants completed an experiment in which they had eight minutes to complete as many MaRs-IB items as possible (items were presented in the same order to all participants). The authors found that performance on this task had good split-half and test-retest reliability. Moreover, they found that participants' ability to solve MaRs-IB puzzles was moderately predictive of their performance on a working memory task and the ICAR matrix reasoning test, indicating good convergent validity. At the conclusion of this study, \cite{chierchia2019matrix} publicly released a summary of the difficulty (i.e. proportion correct responses) of each item template and clone in the MaRs-IB so that other researchers could construct their own test measures of custom difficulty and duration.

Two important limitations in the experiment conducted by \cite{chierchia2019matrix} undermine the utility of these summary statistics, and therefore impede the design of new matrix reasoning tasks using the MaRs-IB. The first is that, given the fixed-order task design,  there was relatively few responses collected for the majority of the items in the MaRs-IB. Indeed, only half (42 of 80 items) were completed by 100 or more participants. As such, one cannot be confident about the true difficulty of a majority of the items (an issue that is exacerbated when considering that each item has six clones). A second issue is participants were able to choose for themselves whether to prioritize accuracy (number of correct responses) or productivity (number of items reached). As a consequence, response data for items that came later in the fixed-order task are increasingly likely to have been produced by participants sacrificing accuracy for speed, thereby biasing the proportion correct measure for these items (see the supplementary materials for evidence of this effect). Therefore, additional investigation of the functioning and psychometric properties of the MaRs-IB is warranted.

Here we provide a more extensive study and psychometric validation of the MaRs-IB. The most important contribution of the current work is the characterization of (almost) every item in the MaRs-IB using item response theory (IRT; \citealt{embretson2013item, de2013theory}). IRT models confer several important advantages for the purposes of psychometric validation. First they provide an estimate of the \textit{difficulty} and \textit{discrimination} of each item, where the latter is an index of how much information an item provides about ability. Using these quantities, IRT models in turn allow experimenters to compute the test information function (TIF), which quantifies the reliability of a test (i.e. an assemblage of items) in measuring performance across a range of ability levels. Finally, IRT models make possible optimal test assembly  \citep{van1998optimal}, or the design of new tests with maximal reliability given experimenter-defined constraints (e.g. test duration, test difficulty).

A second contribution provided here is an investigation of how item characteristics shape the psychometric functioning of items in the MaRs-IB. Using explanatory item response modeling \citep{de2004explanatory, wilson2008explanatory}, we investigate how the number of elements and number of rules impact item functioning (i.e. difficulty, discrimination). We modeled these two properties because they have previously been found to be among the strongest predictors of item difficulty in matrix reasoning tasks \citep{primi2001complexity}. This analysis therefore provides another means to evaluate the success of the MaRs-IB design. Insofar that the number of elements and rules govern the total amount of information that must be stored in working memory in order to correctly solve an item \citep{embretson1998cognitive}, confirmation that they impact performance would also provide insight into the cognitive reasons for individual differences in performance on the MaRs-IB. Finally, explanatory IRT models yield more accurate estimates of item parameters \citep{neuhaus2006separating}, which in turn helps to ensure that new MaRs-IB tests designed using these item parameter estimates function as expected.

%A third contribution is the decomposition of variance in item functioning, into modeled and unmodeled sources, using additive multilevel item structure (AMIS) models \citep{geerlings2011modeling, cho2014additive, lathrop2017item}. First we use AMIS models to quantify the residual variance in difficulty and discrimination across items after accounting for their respective attributes. At the item-level, smaller residual variances indicate that items are functioning as intended. We also use AMIS models to quantify the variance in difficulty and discrimination across item clones. If item clones are to be treated as exchangeable (i.e. able to be used interchangeably), then the residual variance in their item parameters must be negligible. AMIS models therefore provide a second means by which to evaluate the success of the MaRs-IB design.

The remainder of the paper proceeds as follows. First we report a calibration study in which a large sample of participants each completed a subset of items from the MaRs-IB. Using their response data, we then fit a series of multilevel item structure models in order to estimate item parameters for each item (and their clones). Using these same models, we also interrogate the relationship between item attributes and functioning within and cross items in the MaRs-IB. Then we report a second validation study, in which a smaller sample of participants completed one of three new MaRs-IB short-form tests. These tests --- designed using optimal test assembly methods applied to the item parameters estimated during the calibration study --- were found to have good psychometric properties and convergent validity. This second study provides a blueprint for how to construct new MaRs-IB test measures using the item parameter estimates that have been released as part of this study.

All data, code, and model outputs (including the item parameter estimates) are publicly available at: \url{https://github.com/dawlab/mars-irt}.

\clearpage
\section{Calibration Study}

\subsection{Objectives}

The purpose of the first study was to calibrate the items in the MaRs-IB using response data collected from a large number of adult participants from the general population. In particular, we sought to accomplish the following three objectives: (1) to characterize the psychometric properties of the items in the MaRs-IB using item response models; (2) to investigate the associations between item attributes and their psychometric properties, and (3) to determine the equivalence of items with similar complexities, and clones of the same item templates. 

\subsection{Methods}

\subsubsection{Participants} 

A total of N=1584 participants were recruited from the Prolific Academic platform (\url{https://www.prolific.co}) to participate in an online experiment between July - August, 2021. Participants were eligible if they were 18 years or older and lived in the United States. Total study duration was approximately 6.4 minutes (sd = 2.4) per participant. Participants received monetary compensation for their time (rate: \$10 USD/hr), plus a performance-based bonus up to \$0.50 USD. On average, participants earned a total of \$1.30 USD (sd = \$0.10). We offered performance bonuses as they have been found to motivate performance in low-stakes testing environments \citep{duckworth2011role, gignac2018moderate}. This study was approved by the Institutional Review Board of Princeton University (\#7392), and all participants provided informed consent.

To ensure data quality, the data from multiple participants were excluded prior to analysis (see Exclusion Criteria below) leaving the data from N=1501 participants for analysis. In these participants, the majority identified as women (men: N=670; women: N=811; non-binary or other: N=13; rather not say: N=2) and the average age was 28.7 years old (sd = 9.9; range: 18 -- 74). The sample was relatively well-educated with the majority having completed a Bachelor's degree (N=507) or Master's degree or higher (N=322). Comparatively fewer participants completed only some college (N=471), only a high school degree (N=199), or preferred not to say (N=2). 

\subsubsection{Procedure}

After providing consent, participants completed 16 items from the MaRs-IB. The design of the MaRs-IB items have been described previously \citep{chierchia2019matrix}. Briefly, each MaRs-IB item consists of a 3 x 3 matrix. Eight of the nine resulting cells contain abstract shapes, while one cell on the bottom right-hand side of the matrix is empty. Participants are instructed to ``complete the matrix'' by identifying the missing shape from among four possible alternatives. 

The presentation of each item was preceded by a fixation cross, which lasted for 1200 ms. Upon presentation of the item, participants were given up to 30 s to solve the puzzle. After 25 s elapsed, a clock appeared to count down the remaining 5 s. A trial ended when participants responded, or after 30 s had elapsed without response. Before the trials began, participants reviewed instructions and were made to correctly complete three practice items. Participants were instructed to respond carefully, but to guess if they could not solve the puzzle before the timer ran out. The format of the experiment and instructions were adapted from code publicly released as part of the original study (\url{https://app.gorilla.sc/openmaterials/36164}). The task was programmed in jsPsych \citep{de2015jspsych} and distributed using custom web-application software (available at \url{https://github.com/nivlab/nivturk}). 

In order to ensure sufficient sampling of every item (and their clones) in the MaRs-IB, participants were administered a pseudorandomly-selected set of 16 out of 64 total items.\footnote{We did not collect new data for the 16 easiest items in the MaRs-IB (items 1, 2, 3, 4, 5, 7, 8, 9, 32, 33, 38, 41, 43, 48, 57, 68) because performance on these items was at or near ceiling.} Item sets were constructed as follows: We subdivided the 64 items into 16 sets of four based on their dimensionality (a measure of item complexity defined in \cite{chierchia2019matrix}). Participants were randomly assigned one item from each of the 16 sets. As a consequence, all participants received test forms of roughly equal difficulty. Importantly we also counterbalanced the assignment of item clones across participants, such that we had an approximately uniform number of responses available for each item by shape set (1, 2, or 3) and distractor type (minimal difference, MD, or or paired difference, PD; see \cite{chierchia2019matrix} for details). 

\subsubsection{Exclusion criteria}

The data from multiple participants who completed the experiment were excluded prior to analysis for one or more of the following reasons: rapid guessing (responding in less than 3 s)\footnote{A threshold of 3 s was decided on based on pilot data. See the supplementary materials for details.} on four or more trials (N=70); failing to respond on four or more trials (N=8); or minimizing the browser window to a dimension smaller than the minimal requirements (N=7). In total, 83 of 1584 (5.2\%) participants were excluded leaving the data from N=1501 participants available for analysis. Across these participants, each item (64 total) was administered approximately 375 times (sd = 12) and each item clone (384 total) was administered approximately 62 times (sd = 7). 

\subsubsection{Response time analysis}

We investigated the relationship between accuracy and response time using a mixed effects (random intercepts) linear regression model. Trial-wise (log-transformed) response times were predicted as a function of trial accuracy, rest score (participants' observed scores on all other items), and item difficulty (one minus the proportion correct for that item). The mixed-effects model was estimated using the \textit{statsmodels} python package (v0.12.2;  \citealt{seabold2010statsmodels}).

\subsubsection{Item response models}

We employed item response models to characterize the psychometric properties of each item. Specifically, we used additive multilevel item structure (AMIS) models \citep{geerlings2011modeling, cho2014additive, lathrop2017item}. In AMIS models, item parameters are defined according to a hierarchical structure in which item clones are nested in item templates. At the core of all of the AMIS models used here is the three parameter logistic (3PL) item response model, where the probability of a correct response ($y_{ijk} = 1$) for person $i$ on item clone $k$ belonging to item template $j$ is:

\begin{equation} \label{eq:1}
p(y_{ijk} = 1) = \gamma_{jk} + (1-\gamma_{jk}) \cdot \text{logit}^{-1} \left( \alpha_{jk} \cdot \theta_i - \beta_{jk} \right)
\end{equation}

\noindent where $\theta_i$ is the latent ability for person $i$, and $\beta_{jk}$, $\alpha_{jk}$, and $\gamma_{jk}$ are the difficulty, discrimination, and guessing parameters for item clone $k$ of item family $j$. Because estimates of guessing parameters are often unreliable in the absence of very large amounts of response data \citep{han2012fixing}, we fixed the guessing parameter for every item to the nominal guessing rate ($\gamma_{jk} = 0.25$).

The difficulty and discrimination parameters were estimated following an additive multilevel item structure. Concretely, the difficulty of item clone $k$ belonging to item template $j$ was expressed as:  

\begin{equation}
\beta_{jk} = \mu_\beta + \sum_{n=1}^N Q_{jn} \delta_{\beta n} + \epsilon_{\beta j} + \sum_{m=1}^M R_{km} \delta_{\beta m} + \epsilon_{\beta k}
\end{equation}

\noindent To elaborate, item difficulty parameter was modeled as an intercept ($\mu_\beta$), reflecting the average difficulty across all items, and four additional components:

\begin{enumerate}

\item $\sum_{n=1}^N Q_{jn} \delta_{\beta n}$: the effect of item template (level 1) attributes (described below) on difficulty, where $Q_{jn}$ is the value of attribute $n$ for item template $j$, and $\delta_{\beta n}$ is the effect of attribute $n$. This component is the fixed effects contribution to item template difficulty.

\item $\epsilon_{\beta j}$: the template (level 1) residual, or the residual variability in difficulty of the item template unexplained by its attributes. This component is the random effects contribution to item template difficulty.

\item $\sum_{m=1}^M R_{km} \delta_{\beta m}$: the effect of item clone (level 2) attributes on difficulty, where $R_{km}$ is the value of attribute $m$ for item clone $k$, and $\delta_{\beta m}$ is the effect of attribute $m$. This component is the fixed effects contribution to item clone difficulty.

\item $\epsilon_{\beta k}$: the clone (level 2) residual, or residual variability in difficulty of the item clone unexplained by its attributes. This component is the random effects contribution to item clone difficulty.

\end{enumerate}

\noindent So too, item discrimination parameters were expressed as the sum of an equivalent set of components:

\begin{equation}
\alpha_{jk} = \mu_\alpha + \sum_{n=1}^N Q_{jn} \delta_{\alpha n} + \epsilon_{\alpha j} + \sum_{m=1}^M R_{km} \delta_{\alpha m} + \epsilon_{\alpha k}
\end{equation}

\noindent where the interpretation of each component is the same as for item difficulty.

In our analyses, we considered two item template (level 1) attributes, element number and rule number, which have previously been identified as key determinants of item difficulty in matrix reasoning tasks \citep{primi2001complexity}. Element number refers to the number of geometric shapes in each cell of a matrix, whereas rule number refers to the number of relationships that govern the changes of these shapes across cells. In the MaRs-IB, there are four principal rules: size change, color change, position change. Across item templates, element number ranges from one to four (median = 2) and rule number ranges from one to six (median = 3). Importantly, element and rule number are mostly uncorrelated across item templates ($\rho$ = 0.176). The feature and rule number for each item was determined via manual annotation.  

At the clone-level (level 2), we modeled an additional two attributes: distractor type and mean response time. Distractor type refers to whether a clone uses MD-type versus PD-type distractors, whereas mean response time refers to the average time participants spent deliberating on that item clone. We note that, in contrast to all other modeled attributes, average response time is not an intrinsic property of an item. We elected to include it in our models, however, because including response time as a covariate has previously been shown to improve parameter estimation \citep{bertling2018using}. To preclude any collinearity between the attributes, mean response time was orthogonalized with respect to the three other item attributes; as such, any effects of response time reflects residual structure after accounting for all other item attributes.

We deliberately chose not to incorporate shape set (i.e. 1, 2, or 3) as a clone-level attribute. As described above, every item template in the MaRs-IB has three unique versions that differ in the geometric shapes populating its cells. There are 45 unique geometric shapes in total, organized into nine stimulus sets, where each item clone draws a subset of shapes from one of the nine sets (see Figure S2 for examples). As such, the groups in tripartite definition of shape set do not constitute meaningful categories and we therefore do not incorporate them as fixed effects. Furthermore, item clones drawing from the same shape set are not always populated by the same geometric shapes, making complicated the possibility of modeling the nine stimulus sets instead. The most rigorous way of modeling the presence of a given geometric shape on item functioning would be to include each as a binary attribute in estimating item difficulty and discrimination. In order to keep the models simple, we elected to leave shape set unmodeled and accounted for by the residual variability terms instead. 

We fit a series of nested AMIS models in order to identify the model that best predicted participants' response data. The models varied in how item parameters were specified, with each successive model allowing for greater flexibility in item parameter estimation. We fit the models in two waves. In the first, item discrimination was fixed ($\alpha = 1$)\footnote{These models still included a fixed guessing parameter ($\gamma = 0.25$). We also estimated models with no guessing parameter (i.e. one-parameter logistic or Rasch models), but model comparison indicated these models provided worse fits to the data (Table S1).} in order to identify the best structure for the item difficulty independent of item discrimination. In the first wave, we fit three models:

\begin{itemize}

\item Model 1: Item difficulty is a function of item template attributes (level-1) and item clone attributes (level-2). In this model, item difficulty is predicted solely by the fixed effect contribution of item attributes (i.e. element number, rule number, distractor type, response time). 

\item Model 2: Item difficulty is a function of all item attributes (level-1/2) and level-1 residuals. The inclusion of the level-1 residual serves to quantify the magnitude of residual variability in item difficulty across item templates unexplained by the level-1 attributes (i.e. element number, rule number).

\item Model 3: Item difficulty is a function of all item attributes (level-1/2) and level-1/2 residuals. The inclusion of the level-2 residual serves to quantify the magnitude of residual variability in item difficulty across item clones unexplained by the level-2 attributes (i.e. distractor type, mean response time).

\end{itemize}

In the second wave of model fitting, item discrimination was now a free parameter to be estimated. Moreover, in the second set of models, item difficulty parameters were specified according to the best-fitting model from the first wave. In the second wave, we fit an additional three models:

\begin{itemize}

    \item Model 4: Item discrimination is a function of item template attributes (level-1) and item clone attributes (level-2). In this model, item discrimination is predicted solely by the fixed effect contribution of item attributes (i.e. element number, rule number, distractor type, response time). 
    
    \item Model 5: Item discrimination is a function of all item attributes (level-1/2) and level-1 residuals. The inclusion of the level-1 residual serves to quantify the magnitude of residual variability in item discrimination across item templates unexplained by the level-1 attributes (i.e. element number, rule number).
    
    \item Model 6: Item discrimination is a function of all item attributes (level-1/2) and level-1/2 residuals. The inclusion of the level-2 residual serves to quantify the magnitude of residual variability in item discrimination across item clones unexplained by the level-2 attributes (i.e. distractor type, mean response time).
    
\end{itemize}

The AMIS modeling framework provides a natural means of accomplishing the three goals of the calibration study. Through any of the models above, we obtain estimates of the the psychometric properties (i.e. difficulty, discrimination) of every item template and clone in the MaRs-IB (Aim 1). Through the level-1 fixed effects, we are able to test for associations between item complexity (i.e. element and rule number) and item functioning (Aim 2). We predicted that both attributes would be positively associated with item difficulty; that is, all else equal, items with either a great number of elements and rules would be more difficult. In contrast, we had no hypotheses about the impact of these attributes on item discrimination. 

Finally, through both the level-2 fixed and random effects, we can evaluate whether item clones in the MaRs-IB are exchangeable (Aim 3). For example, an association between distractor type and item functioning would entail that clones are not psychometrically equivalent. Moreover, if the level-2 residual variance in item difficulty or discrimination were non-negligible, that would similarly entail that item clones are not psychometrically equivalent due to one or more unmodeled features (e.g. low-level perceptual properties).

\subsubsection{Person-level effects}

In each of the six models described above, we incorporated person-level attributes as predictors of latent ability ($\theta_i$) following an explanatory item response modeling approach \citep{wilson2008explanatory}: 

\begin{equation} \label{eq:4}
\theta_i = \sum_{p=1}^P X_{ip} \rho_p + \epsilon_i    
\end{equation}

\noindent where $X_{ip}$ is the value of attribute $p$ for person $i$, $\rho_p$ is the partial correlation of person attribute $p$, and $\epsilon_i$ is variance in ability unexplained by the person-level attributes. 

We estimated person-level abilities as a function of four covariates: age, gender, mean response time, and $\Delta$ response time. This last term reflects the degree of change in a participant's deliberation time as a function of item difficulty (defined as one minus the proportion of correct responses for that item). Mean response time and $\Delta$ response time were calculated for each participant simultaneously through linear regression, where a given participant's (log-transformed) response times were regressed against an intercept and item difficulty. All covariates were standard-scored prior to analysis with the exception of gender, which was binary coded (male = -0.5, female = 0.5).

\subsubsection{Model Fitting}

All models were estimated within a Bayesian framework using Hamiltonian Monte Carlo as implemented in Stan (v2.22) \citep{carpenter2017stan}. For all models, four separate chains with randomised start values each took 7500 samples from the posterior. The first 5,000 samples from each chain were discarded. As such, 10,000 post-warmup samples from the joint posterior were retained. The $\hat{R}$ values for all parameters were less than 1.01, indicating acceptable convergence between chains, and there were no divergent transitions in any chain. 

During estimation, the item discrimination parameters were restricted to be in the range $\alpha_{jk} \in [0, 5]$. To ensure the identifiability of all models, person abilities were constrained to have a mean of zero and a variance of 1.0. Specifically, the residual variance of latent ability was specified as $V(\epsilon) = 1 - \sum \rho_p^2$.

\subsubsection{Model comparison}

The goodness of fit of the models was compared using Bayesian leave-one out cross-validation \citep{vehtari2017practical}, which has been found to perform better than more traditional information criteria for comparing item response models \citep{luo2017performances}. We computed the conditional leave-one-cluster out (LOCO) cross-validation \citep{merkle2019bayesian}, which measures a model's ability to generalize to held out items (rather than held out responses or held out participants) --- i.e., for inferring generalization to additional items constructed using the same features rather than to additional participants sampled from the same population.

\subsubsection{Goodness-of-fit}

The fit of the best-fitting model to the data was evaluated using posterior predictive model checking \citep{gelman1996posterior, levy2017bayesian}. A sample of predicted responses was generated for each sample of simulated parameters and a posterior predictive $p$ (PPP) value was computed based on two discrepancy statistics: a chi-square $\left( \chi^2_{NC} \right)$ discrepancy measure based on the observed score distribution \citep{sinharay2006posterior} and the standardized generalized dimensionality discrepancy measure (SGDDM; \citealt{levy2015standardized}). 

The $\chi^2_{NC}$ discrepancy statistic is a measure of item fit based on the comparison of observed and expected proportions of participants at each score level (here we ignore scores of zero and one, which comprised 7 total participants). A global measure of fit at the test level is obtained by summing the discrepancy values over the groups. In turn, SGDDM measures the mean absolute conditional correlation between all pairs of items; that is, the SGDDM is an index of residual inter-item correlations unexplained by the model. The SGDDM is important in this context as it tests for local dependence across items, the presence of which might otherwise lower the efficiency of future test forms via the inclusion of nonindependent or redundant items. For both discrepancy statistics, the PPP value is the proportion of draws in which the posterior predictive discrepancy is equal to or higher than the realized discrepancy. A poor model fit to the data is indicated when the PPP-values are extreme (PPP $\leq$ 0.05 or PPP $\geq$ 0.95).

\subsection{Results}

\subsubsection{Descriptive statistics}

On average, participants completed 9.6 of 16 items correctly (sd = 3.1, IQR = 8.0 -- 12.0) (Figure \ref{fig:fig01}a). Timing out occurred on only 1.8\% of trials. A total of 43 participants (2.9\%) performed below chance (i.e. fewer than four items correct), while only 11 participants (0.7\%) solved all 16 of their items. Thus, over 95\% of observed scores were in a reasonable range. These results corroborate those reported by \cite{chierchia2019matrix} in that there were no obvious ceiling or floor effects in performance, and that the majority of participants were sufficiently motivated to participate in spite of the low-stakes testing environment.  

Across all 384 item clones, the average proportion correct was 0.60 (sd = 0.19, IQR = 0.45 - 0.74; Figure \ref{fig:fig01}b). A total of 12 item clones (3.1\%) exhibited performance levels beneath chance, and only 22 item clones (5.7\%) had performance levels near ceiling ($\geq$ 90\%). Across items, the proportion of correct responses was negatively correlated with rule number ($\rho$ = -0.482, p < 0.001). These results further corroborate those reported by \cite{chierchia2019matrix} in that the MaRs-IB exhibited good functioning with performance levels for only a few item clones at ceiling or floor. 

\subsubsection{Response time analysis}

The results of the linear mixed-effects model of response times are summarized in Table \ref{table:1}. On average, participants spent 15.9 s (sd = 7.2 s) per item. There were significant main effects of accuracy ($\beta = 0.067$, $t = 9.84$, $p < 0.001$), rest score ($\beta = 0.125$, $t = 16.95$, $p < 0.001$), and item difficulty ($\beta = 0.137, t = 44.33, p < 0.001$). In other words, response times were slower on average for correct responses, better- performing participants, and more difficult items. The positive association between response time and accuracy is consistent with speed-accuracy trade-offs; that is, slower responses are associated with more careful work. This interpretation is corroborated by the main effect of rest score, such that more accurate participants were slower overall. 

There were also significant accuracy by difficulty ($\beta = 0.097$, $t = 14.34$, $p < 0.001$) and accuracy by rest score interactions ($\beta = -0.064$, $t = -8.76$, $p < 0.001$). Correct responses were even slower for more difficult items, but faster for better-performing participants. There was also a significant difficulty by score interaction ($\beta = 0.032$, $t = 6.55$, $p < 0.001$), such that better-performing participants exhibited even slower responses for more difficult items. This pattern of response time results is visible in Figure \ref{fig:fig01}c. The three-way interaction term was not significant. 

In sum, we found evidence of speed-accuracy trade-offs in performance in this sample. Accurate responses and more accurate participants were both slower overall. Notably, better-performing participants demonstrated adjustments to their response times as function of item difficulty; whereas the lowest scoring participants maintained the same work rate regardless of item difficulty, the highest scoring participant exhibited the largest slowing in responses as items became more challenging. We return to this pattern of results in the General Discussion. 

\subsubsection{Model comparison}

The results of the model comparison is summarized in Table \ref{table:1}. Of the models fitted in the first wave, which varied only in their specification of item difficulty, Model 3 exhibited the best fit to the data. Indeed, Model 3 demonstrated a considerable improvement in fit over Model 1 ($\Delta$ LOCO = 1155.8, se = 60.1) and Model 2 ($\Delta$ LOCO = 376.5, se = 34.5), indicating the presence of non-negligible residual variance in item difficulty across both item templates and clones. As the best-fitting model in the first wave, Model 3 served as the starting point in the second wave of model fitting.

In the second wave of model fitting, in which models varied only in their specification of item discrimination, Model 5 demonstrated the best fit to the data. By comparison, Model 5 demonstrated smaller improvements in fit over Model 4 ($\Delta$ LOCO = 5.12, se = 4.07) and Model 6 ($\Delta$ LOCO = 3.29, se = 1.84). This result indicates the presence of non-negligible residual variance in item discrimination across item templates but not clones. It must be noted these differences in LOCO values from each model to the next were each within two standard errors of the mean, indicating only weak predictive improvement of Model 6 over the others \citep{vehtari2022cv}. As such we will select Model 5 as the best-fitting model overall, but proceed in discussing it with caution. We also note that Model 6 fit clearly better than Model 4 ($\Delta$ LOCO = 85.91, se = 6.33, supporting the inclusion of varying item discrimination parameters). We also note that all second-wave models exhibited better fits to the data than the first-wave models, supporting the inclusion of item discrimination parameters.

\subsubsection{Aim 1: Quantify the psychometric properties of items in the MaRs-IB}

Across all items, the average item difficulty was $\mu_\beta = 0.177$ (95\% HDI = 0.118 -- 0.238). There was considerable variability in item difficulty across items (sd = 1.431, 95\% HDI = 1.370 -- 1.492), with the smallest  and largest item difficulty parameters spanning a wide range ($\beta_{\min}$ = -3.704, 95\% HDI = -4.687 -- -2.775; $\beta_{\max}$ = 3.497, 95\% HDI = 2.481 -- 4.635). For an average ability participant ($\theta = 0$), these parameter estimates translate to an expected average accuracy of 58.4\% across all items, and ceiling (chance-level) performance for the least (most) difficult item.

In turn, the average item discrimination was $\mu_\alpha = 1.298$ (95\% HDI = 1.212 - 1.385). Variability in item discrimination was modest by comparison (sd = 0.221, 95\% HDI = 0.121 - 0.322). Item difficulty and discrimination were uncorrelated across items ($\rho$ = -0.060, 95\% HDI = -0.379 -- 0.250). Following convention \citep{baker2017basics}, the items in the MaRs-IB exhibited medium ($\alpha \in$ 0.65 - 1.34) to high ($\alpha \in$ 1.35 â€“ 1.69) levels of discrimination and are thus suitable for measuring matrix reasoning ability.

\subsubsection{Aim 2: Measuring the association between item complexity and functioning}

Next we inspected the associations between item complexity (i.e. number of elements and number of rules) and item functioning. As predicted, a one-unit change in the number of elements was associated with an increase in item difficulty ($\delta_{\beta_1}$ = 0.579, 95\% HDI = 0.389 -- 0.770), or an approximate 10.6\% reduction in accuracy for a participant of average ability. Similarly, a one-unit change in the number of rules was also associated with an increase in item difficulty ($\delta_{\beta_2}$ = 0.514, 95\% HDI = 0.378 -- 0.663), or a 9.5\% reduction in accuracy. The difference in coefficients was not credibly different than zero ($\delta_{\beta_1} - \delta_{\beta_2}$ = 0.065, 95\% HDI = -0.189 -- 0.319), indicating that both attributes exert approximately equal impact on item difficulty.

Together the number of elements and number of rules explained approximately 67.6\% of the variance in difficulty across the 64 item templates. In other words, the majority of variability in difficulty across item templates can be attributed to their complexity. Individually the number of elements and number of rules explain 29.5\% and 40.6\% of the variance in item template difficulty, respectively. Across all 384 item clones, these two attributes explain 38.6\% of the variance in difficulty.

Only the number of rules was associated with item discrimination: a one-unit change in rule number was associated with a marginal increase in discrimination ($\delta_{\alpha_2}$ = 0.020, 95\% HDI = 0.003 -- 0.037). There was not a credible association between the number of elements and discrimination ($\delta_{\alpha_1}$ = -0.015, 95\% HDI = -0.036 -- 0.008). These two attributes explained 31.9\% of the variance in discrimination across item templates. 
\subsubsection{Aim 3:}

\subsubsection{Objective 3: Investigating the exchangeability of item clones}

Contrary to the assumption that item clones are exchangeable, we found an association between distractor type and item difficulty ($\delta_{\beta_3}$ = 1.105, 95\% HDI = 0.940 -- 1.269). Items with MD distractors were associated with an estimated 19.9\% reduction in accuracy compared to their equivalent items with PD distractors. Together, number of elements, number of rules, and distractor type explained slightly over half (52.0\%) of the variance in item difficulty. Regardless, a model that included a clone-level random effects term for item difficulty was preferred to models without. Therefore, the results indicate that item clones are not equally difficult and are thus not exchangeable. 

The estimated item difficulty per clone, organized by distractor type and shape set, is presented in Figure \ref{fig:fig02}. The relative increase in difficulty for items with MD-type distractors compared to PD-type distractors is visually apparent (Figure \ref{fig:fig02}a). In contrast, the pattern of item difficulties across clones by shape set is more complicated (Figure \ref{fig:fig02}b). After controlling for distractor type and mean response time, the magnitude of the clone-level residual variance is $\epsilon_{\beta k}$ = 0.620 (95\% HDI = 0.538 -- 0.701). Across the 128 unique sets of three clones, organized by item and distractor type, that level of residual variance translates into an average difference in difficulty of 0.752 (95\% HDI = 0.630 -- 0.877) between the easiest and hardest item clones of each triad, an effect that is two-thirds as large as the effect of distractor type. Furthermore, the shape set membership of the most difficult clone varies considerably across the 128 sets of clones (shape set 1 = 39; shape set 2 = 32; shape set 3 = 57). Thus, the sources of residual variability in item difficulty across clones is complex and likely reflect the idiosyncratic contributions of their low-level perceptual features. 

Turning to item discrimination, we did not find a credible association between discrimination and distractor type ($\delta_{\alpha_3}$ = -0.029, 95\% HDI = -0.065 -- 0.005). Moreover, a model that included a clone-level random effects term for item discrimination was not preferred to a simpler model without. As such, there is not sufficient evidence to refute that the assumption that item clones are similarly discriminating. We note this result should be interpreted with caution given the relatively weak predictive improvement of the best-fitting model compared to a model with a clone-level random effects term.

Finally, we inspected the relationship between mean response time and item functioning. As expected, there was a positive association between mean response time and item difficulty ($\delta_{\beta_4}$ = 0.541, 95\% HDI = 0.414 -- 0.671). That is, response times were slower on average for more difficult items. There were not, however, a credible associations between item discrimination and average response time ($\delta_{\alpha_4}$ = -0.010, 95\% HDI = -0.030 -- 0.009).

\subsubsection{Person-level effects}

In the best-fitting model, there were several credible associations between person-level attributes and ability. There was a negative association between age and ability ($\rho_1$ = -0.299, 95\% HDI = -0.353 -- -0.246). There was also a small association between gender and ability ($\rho_2$ = 0.128, 95\% HDI = 0.078 -- 0.182), such that women performed marginally better than men. Average response time was positively associated with ability ($\rho_3$ = 0.427, 95\% HDI = 0.374 -- 0.478), as was $\Delta$ response time ($\rho_4$ = 0.368, 95\% HDI = 0.313 -- 0.424). In other words, participants with higher levels of ability spent on average more time on each item and deliberated for longer on more difficult items. Together, these four person attributes explained 50.3\% of the variance in ability. These results are consistent with a speed-accuracy trade-off --- higher levels of ability in this sample reflect in part a tendency to slow down and respond carefully overall and increasingly for more challenging items.  

\subsubsection{Goodness-of-fit}

Finally, we validated the fit of the best-fitting model using two posterior predictive model checking measures. The PPP-value for chi-square discrepancy statistic did not exceed the critical threshold ($\chi^2_{NC}$ = 15.904, PPP-value = 0.297), indicating that the model was sufficiently able to reproduce the distribution of observed scores. The PPP-value for the SGDDM statistic did exceed the critical threshold (PPP-value = 0.019). However, the mean of the realized SGDDM values (SGDDM = 0.0180) were only marginally larger than the mean of the posterior predicted SGDDM values (SGDDM = 0.0175). That is, the residual inter-item correlations in the data unexplained by the best-fitting model were small on average and only slightly larger than what we would expect to observe by chance. As such, there is little evidence for local dependence in the data. Overall then, we can conclude the best-fitting model provides an adequate fit to the data.

\subsection{Discussion}

In this first study, we recruited a large sample of adults to complete items from the MaRs-IB, making sure that each item (and its clones) were completed by a sufficient number of participants. Using multilevel item structure models, we then estimated the psychometric properties (i.e. difficulty and discrimination) of each item. Corroborating the results of an initial investigation of the MaRs-IB \citep{chierchia2019matrix}, we found that the items in the MaRs-IB vary greatly in their difficulty -- a prerequisite for measuring nonverbal reasoning across the ability spectrum. We also found that the items in the MaRs-IB exhibited medium-to-large levels of discrimination, similar to what has been found in investigations of other of matrix reasoning tasks \citep{chiesi2012using, chiesi2012item, van2013shortened}.

We also investigated how the structural characteristics of items impact their functioning. We found that the number of elements and number of rules were both positively associated with item difficulty. These two attributes exerted approximately equal influence on difficulty (i.e. a one-unit change in either was independently associated with a roughly 10\% reduction in performance). This finding is interesting in light of previous investigations of nonverbal reasoning tasks, which have found either the number of elements \citep{bethell1984adaptive} or number of rules \citep{mulholland1980components} to be more determinant of item difficulty. One possible reason for this discrepancy is that these two attributes are largely uncorrelated ($\rho$ = 0.176) across items in the MaRs-IB. Together the number of elements and number of rules explained 67.6\% of the variance in item difficulty across item templates (and 38.6\% of the variance across item clones), which is in line with previous investigations of matrix reasoning tasks \citep{carpenter1990one, matzen1994error}. These findings further validate the design of the MaRs-IB insofar that item complexity strongly determines item difficulty.

Finally, we investigated whether the item clones in the MaRs-IB are psychometrically equivalent and exchangeable. Crucially, we found repeated evidence that item clones are not equally difficult. Unexpectedly, and contrary to the results of \cite{chierchia2019matrix}, distractor type emerged as a substantial predictor of item difficulty (i.e. items with MD-type distractors are associated with an almost 19.9\% reduction in accuracy compared to the same items with PD-type distractors). This discrepancy in findings between studies may reflect our larger sample size (N=1501 versus N=659 participants) or population (adults-only versus a mixture of adults, adolescents, and children), and deserves further investigation. Even after controlling for the number of elements, number of rules, and distractor type, we found non-negligible residual variability in item difficulty across clones that were difficult to characterize and likely reflect both the additive and interactive effects of low-level perceptual features. Regardless, our results clearly indicate that item clones in the MaRs-IB are not psychometrically equivalent and should not be treated as exchangeable. 

In summary, the results of the calibration study confirm the MaRs-IB to be a useful and psychometrically valid resource for measuring matrix reasoning ability. The items vary considerably in their difficulty, and all posses medium-to-large levels of discrimination, thereby making the MaRs-IB suitable for measuring matrix reasoning across a wide range of ability levels. That item clones are not equivalently difficult, and should not therefore be treated as exchangeable, does not invalidate this conclusion. The item parameter estimates derived as part of this study are therefore suitable for use in designing new MaRs-IB measures with both good reliability and potential for reuse, as we demonstrate in the next study. 

\section{Validation Study}

\subsection{Objectives}

The purpose of the second study was to provide a demonstration of how to use the item parameter estimates derived in the previous study to design new MaRs-IB measures. Specifically, we use optimal test assembly methods to construct three parallel short form measures with maximal reliability under time length constraints. 

\subsection{Optimal test assembly}

Given a starting point of 64 item templates with six clones each, there is a vast number of possible MaRs-IB test forms we could construct. Returning to our motivating problem --- the availability of matrix reasoning measures with the potential for reuse --- we decided to construct three abbreviated parallel short form measures. To do so, we used mixed integer programming \citep{der2005wj} to maximize the test information function (TIF) of each short form subject to the following constraints:

\begin{enumerate}

    \item Each short form was required to contain 12 items. This was chosen to minimize the administration time of a given form (2-4 minutes on average) while achieving sufficient test score reliability (Cronbach's $\alpha \geq 0.8$).
    
    \item All short forms were required to be made up of clones from the same items. This constraint was adopted in order to maximize the similarity of the short forms.
    
    \item Clones selected for inclusion could either use MD-type or PD-type distractors, but not both. This constraint was adopted in order to prevent item redundancy (i.e. including the same puzzle twice in one short form).
    
    \item The difference in TIFs between short forms should be minimized. This constraint was adopted to ensure that each short form had approximately equal psychometric properties (i.e. equivalent measurement reliability across ability levels). 
    
\end{enumerate}

\noindent The TIF was maximized at five ability levels ($\theta = -1.0, -0.5, 0.0, 0.5, 1.0$), which previous simulation studies have shown to be generally sufficient \citep{der2005wj}. Solutions to the mixed integer programming problem were found using the \textit{mip} python package (v1.13.0; \citealt{santos2020mixed}).

The results of the test assembly are presented in Figure \ref{fig:fig03}. As expected, the MIP solver selected for items that were more discriminating on average (Figure \ref{fig:fig03}a). Though this was not an explicit constraint, the test characteristic curves (TCCs) of the three short forms are markedly similar (Figure \ref{fig:fig03}b). Based on the TCCs, the expected observed score for each measure is 7.7 out of 12 items. (Note that, because the lowest ability participants are expected to guess an item correctly 25\% of time, the lower asymptote of the TCCs is at a score of 3.) Finally, the TIFs across the three short forms are remarkably similar (Figure~\ref{fig:fig03}c), as is expected given the last assembly constraint. As such, we should expect each short form to have approximately equal precision in measuring matrix reasoning ability across the spectrum of ability. Indeed, the simulated marginal score reliability for each test form are similar (short form 1: $\alpha = 0.794$; short form 2: $\alpha = 0.795$; short form 3: $\alpha = 0.792$). 

Next, we proceeded to administer each short form to a second sample of participants. Our aim was to verify that observed scores on each short form conformed to our expectations based on the TCCs and TIFs. Doing so would provide evidence that we were successful in calibrating the functioning of the items in the MaRs-IB in the previous study. We also sought to demonstrate convergent validity by correlating performance on the MaRs-IB short forms with performance in the same participants on an abbreviated version of the Raven's progressive matrices \citep{bilker2012development}.

\subsection{Methods}

\subsubsection{Participants}

A total of N=347 participants were recruited from the Prolific Academic platform to participate in an online behavioral experiment in November, 2021. Participants were eligible if they currently resided in the United States and had not participated in the calibration study. Study duration was on average 10.8 minutes (sd = 4.6). Participants received monetary compensation for their time (rate USD \$10/hr) plus a bonus up to \$0.75 based on task performance. Participants earned on average \$2.10 USD (sd = \$0.18). This study was approved by the Institutional Review Board of Princeton University (\#7392), and all participants provided informed consent. 

To ensure data quality, the data from multiple participants were excluded prior to analysis (see Exclusion Criteria below) leaving the data from N=300 participants for analysis. The majority of participants identified as women (men: N=138; women: N=147; non-binary or other: N=13; rather not say: N=2). Participants were on average 35.0 years old (sd = 13.2; range: 18 -- 76). The sample was relatively well-educated, with the majority having completed a Bachelor's degree (N=121) or Master's degree or higher (N=45). By comparison, fewer participants endorsed having completed only some college (N=100), only a high school degree (N=33), or preferred not to say (N=1). 

\subsubsection{Procedure}

The study was divided into three parts. After providing consent, participants first completed three short surveys: the 10-item need for cognition survey \citep{chiesi2018applying}, the 8-item PROMIS Cognitive Function-Short Form (8a) \citep{iverson2021normative}, and the 8-item subject numeracy scale \citep{fagerlin2007measuring}. These measures were included as part of secondary analyses  to explore the associations between personality and matrix reasoning performance (all pairwise correlations are reported in Table S2 of the supplementary materials). Afterwards participants completed one of the three 12-item MaRs-IB short-form measures. The administration procedure of the short-forms was identical to that in the first study. Finally, participants completed the 9-item abbreviated Raven's progressive matrices (RPM form A; \citealt{bilker2012development}). To maximize consistency in administration, the presentation format and instructions for the RPM task was the same as for the MaRs-IB test forms.

\subsubsection{Exclusion criteria}

To ensure data quality, the data from multiple participants were excluded prior to analysis for one or more of the following reasons: failing to complete all three sections of the experiment (N=17); failing one or more attention checks \citep{zorowitz2021inattentive} embedded in the self-report measures (N=21); experiencing technical difficulties during the experiment (N=11); rapid guessing on four or more items (N=3); or failing to respond on four or more items (N=1). In total, 47 of 347 (13.5\%) participants were excluded leaving the data from N=300 participants for analysis.

\subsubsection{Analysis}

To measure the convergent validity between the MaRs-IB and RPM measures, we fit an additional explanatory item response model. Here participants' ability on the MaRs-IB short forms were estimated as a function of their gender, age, and observed score on the RPM (Eq \ref{eq:4}). The probability of a participant correctly solving a MaRs-IB was defined according to the 3PL model (Eq \ref{eq:1}), where the difficulty and discrimination parameters for an item were fixed to the posterior mean of the estimates obtained in the previous study. Thus, the model estimates the partial correlation between RPM performance and MaRs-IB ability controlling for the age and gender of the participants, and the differences in item functioning across test forms.

The IRT model was estimated within a Bayesian framework using Hamiltonian Monte Carlo as implemented in Stan (v2.22) \citep{carpenter2017stan}. Four separate chains with randomised start values each took 7500 samples from the posterior. The first 5,000 samples from each chain were discarded. As such, 10,000 post-warmup samples from the joint posterior were retained. The $\hat{R}$ values for all parameters were equal to or less than 1.01, indicating acceptable convergence between chains, and there were no divergent transitions in any chain. 

We calculated the ordinal reliability of the MaRs-IB and RPM short-form measures using the \textit{lavaan} \citep{lavaan} and \textit{semTools} \citep{semtools} packages available in R.

\subsection{Results}

Performance on the MaRs-IB short-forms is summarized in Table \ref{table:3}. On average, participants completed the measure in 174 s (sd = 54.0 s) and correctly solved 8.0 of 12 items (sd = 2.5). The distribution of observed scores was right-shifted (Figure~\ref{fig:fig04}), as was expected given the nominal guessing rate. A one-way ANOVA comparing the average score across the three short-forms was not statistically significant (F(2,297) = 0.253, p = 0.777). Importantly, the marginal score reliability of each measure was satisfactory (all $\alpha > 0.75$). In sum, performance on each MaRs-IB test measure conformed to expectations, indicating that item functioning was adequately calibrated in the previous study.

Performance on the abbreviated RPM measure is also summarized in Table \ref{table:3}. On average, participants completed the measure in 125 s (sd = 40.1 s) and responded correctly on 4.5 of 9 items (sd = 2.0). The proportion of correct responses on on the RPM was significantly lower than that observed for the MaRs-IB ($t = 13.028$, $p < 0.001$), suggesting that the RPM is more difficult or less biased by guessing.\footnote{In contrast to the MaRs-IB, items in the RPM have either six or eight response options. As such, the nominal guessing rate is expected to be lower for the RPM.} The marginal score reliability for the RPM was also adequate.

The partial correlation between observed scores on the RPM and ability on the MaRs-IB was $\rho = 0.527$ (95\% HDI = 0.457 - 0.577). Considering the attenuation of correlations between two imperfect measures \citep{spearman1961proof}, we should not expect correlations much larger than $\rho \approx 0.64$ (or $0.80 \times 0.80$). Therefore, this result indicates satisfactory convergent validity between the two matrix reasoning measures. Briefly, in this second sample, neither age nor gender was credibly associated with performance on the MaRs-IB short forms (age: $\rho$ = -0.123, 95\% HDI = -0.253 -- 0.013; gender: $\rho$ = -0.025, 95\% HDI = -0.160 -- 0.108). 

To summarize, we used the estimated item parameters in conjunction with optimal assembly methods to construct three parallel MaRs-IB short forms. As expected, these new measures were quick to administer, exhibited score distributions in line with model predictions, and demonstrated good internal reliability and convergent validity with a second matrix reasoning measure. Together, these results highlight our success in accurately calibrating the functioning of MaRs-IB items, and consequently the feasibility of constructing new matrix reasoning measures.

\section{General Discussion}

Here we have provided the most comprehensive investigation of the MaRs-IB to date. Using item structure models fit to data collected from a large sample of adult participants, we found that the MaRs-IB possesses many desirable psychometric properties. The items in the MaRs-IB span a considerable range of difficulty and exhibit medium-to-high levels of discrimination. The MaRs-IB is therefore suitable for measuring matrix reasoning across the ability spectrum in the adults. We also verified that the design of the MaRs-IB was in part successful. As intended, item attributes such as feature number and rule number were positively associated with, and explained much of the variability, in item difficulty. We also found some limitations in the design of the MaRs-IB. For example, distractor type was also associated with item difficulty. Moreover, we also found non-negligible residual variance in item difficulty across clones after controlling for item attributes. As a consequence, item clones in the MaRs-IB cannot therefore be treated as psychometrically equivalent and exchangeable. 

With the estimated item parameters, we then constructed three new MaRs-IB short-form measures using optimal test assembly methods. These abbreviated test measures were designed to measure matrix reasoning ability with maximal reliability in the general adult population under several desirable constraints (i.e. 12 item limit, administration time of 2-4 minutes). In a second sample of participants, we found that each short form measure had adequate reliability (Cronbach $\alpha \approx 0.8$) and good convergent validity with an abbreviated Raven's progressive matrices measure. Moreover, the observed scores on the short-forms followed the expected distribution as predicted by their test characteristic curves. These results demonstrate the success of the item calibration study in producing accurate estimates of the item parameters. 

Of additional note are the associations we found between accuracy and response time. Accurate responses and more accurate participants were slower on average, consistent with a speed-accuracy trade-off account of performance \citep{heitz2014speed}. Interestingly, we also found an interaction between participant ability and item difficulty: whereas the worst-performing participants maintained approximately the same work rate irrespective of item difficulty, the best-performing participants were even slower as items became more challenging. These results are consistent with a persistence interpretation of ability, wherein good task performance in part reflects a willingness to invest time in the solution process and poor task performance reflects a tendency to give up sooner \citep{ranger2021effects}. As such, the MaRs-IB may be suitable not only for measuring matrix reasoning ability but also mental effort costs \citep{kool2018mental}, opportunity costs \citep{otto2019opportunity}, or other motivational factors \citep{duckworth2011role} related to the tendency to exert effort or give up. The use of more sophisticated models may help to disentangle the relative contributions of latent ability and persistence to performance on the MaRs-IB \citep{ranger2014accumulator}.

The current investigation of the MaRs-IB is not without its own limitations. One notable limitation is our sample. Here we analyzed response data collected from an online adult sample which was relatively young and well-educated. We cannot guarantee that the psychometric properties of the MaRs-IB found here will generalize to other populations or testing contexts. Future researchers may consider replicating the current study in other populations of interest (e.g. children, clinical samples). By using item response models here, however, we make possible the opportunity for future IRT ``linking'' studies. IRT linking describes a set of methods to establish the comparability of item and ability parameters estimated from data collected from two or more groups that differ in ability \citep{lee2018irt}. Future studies utilizing these methods could provide insight not only in how the functioning of the MaRs-IB might change in across populations, but also how matrix reasoning ability differs across populations.

A second limitation is in our relatively limited investigation of sources of variance in item functioning. Here we investigated the contributions of only three item attributes. There were many item properties that we elected not to model (e.g. type of changing relation such as color change or shape change). Previous investigations of matrix reasoning tasks have proposed other taxonomies of item attributes that affect item functioning \citep{carpenter1990one}. Incorporating additional attributes into our models may have further increased the precision of our parameter estimates and might  provide additional knowledge for guiding test assembly. Future studies should consider more deeply investigating the possible relationships between other attributes of items in the MaRs-IB and their functioning. 

In conclusion, we have provided an psychometric validation of the MaRs-IB, finding that it is suitable for measuring matrix reasoning ability in the general adult population. We hope that the materials and results presented here will encourage researchers interested in measuring matrix reasoning using the MaRs-IB in their studies. 

\section{Acknowledgements}
\noindent This work was funded in part by the National Center for Advancing Translational Sciences (NCATS), a component of the National Institute of Health (NIH), under award number UL1TR003017, and by an NSF Graduate Research Fellowship.

\bibliography{main}

\begin{table}[]
\centering
\begin{tabular*}{\textwidth}{lc@{\hskip 6mm}r@{\hskip 6mm}c}
\toprule
Predictor &  Coef. (se) & Z-score (p-value) & 95\% CI  \\
\midrule
Accuracy & 0.039 (0.007) &   5.367 (<0.001) &  0.025 -- 0.053  \\
Score &  0.167 (0.009) &  19.204 (<0.001) &  0.150 -- 0.184  \\
Difficulty & 0.073 (0.005) &  13.292 (<0.001) &  0.062 -- 0.083  \\
Accuracy x Score & -0.064 (0.007) &  -8.764 (<0.001) & -0.078 -- -0.050  \\
Accuracy x Difficulty & 0.097 (0.007) &  14.351 (<0.001) &  0.084 -- 0.110  \\
Score x Difficulty &  0.032 (0.005) &   6.549 (<0.001) &  0.023 -- 0.042  \\
Accuracy x Score x Difficulty &  0.012 (0.006) & 1.955 (0.051) & -0.000 -- 0.025  \\
\bottomrule
\end{tabular*}
\caption{\label{table:1}\normalfont Summary of the mixed effects linear regression model predicting log-transformed response time as a function of trial accuracy, participant rest score (total correct on all other items), and item difficulty (one minus the proportion correct for an item). \newline lme4 syntax: log(RT) $\sim$ accuracy * score * difficulty + (1 | subject).}
\end{table}

\begin{table}
    \centering
    \begin{tabular*}{1.02\textwidth}{cccccccccccr}
    \toprule
    && \multicolumn{3}{c}{Difficulty} && \multicolumn{3}{c}{Discrimination} && \multicolumn{2}{c}{LOO-CV} \\
    \cmidrule(lr){3-5}\cmidrule(lr){7-9}\cmidrule(lr){11-12}
    Model && {\small FE-1/2} & {\small RE-1} & {\small RE-2} && {\small FE-1/2} & {\small RE-1} & {\small RE-2} && psis-loco & $\Delta$ psis-loco (se) \\
    \midrule
    1 && X &   &   &&   &   &   && 27741.4 & 1326.87 (62.88) \\
    2 && X & X &   &&   &   &   && 26962.1 & 547.56 (37.79) \\
    3 && X & X & X &&   &   &   && 26585.5 & 171.05 (12.65) \\
    4 && X & X & X && X &   &   && 26419.6 & 5.12 (4.07) \\
    5 && X & X & X && X & X &   && 26414.5 & \multicolumn{1}{c}{-} \\
    6 && X & X & X && X & X & X && 26417.8 & 3.29 (1.84) \\
    \bottomrule
    \end{tabular*}
    \caption{\label{tab:2}\normalfont Comparison of item response models fit to the MaRs-IB response data. The columns under difficulty and discrimination indicate the specification of those parameters for each model (i.e. the presence of level-1/2 fixed effects, level-1 random effects, and level-2 random effects). LOO-CV values are presented in deviance scale (i.e. smaller values indicate better fit). Abbreviations: PSIS = Pareto-smoothed importance sampling; LOCO = leave-one-cluster-out.}
    \label{table:2}
\end{table}

\begin{table}
    \centering
    \begin{tabular*}{\textwidth}{llcccc}
    \toprule
    Measure & Sample & Task Time (sd) & Mean Score (sd) & IQR & Reliability ($\alpha$) \\
    \midrule
    MaRs-IB SF-1 & N=103 & 174.9 s (53.3) & 7.9 (2.4) & 7 - 10 & 0.761 \\
    MaRs-IB SF-2 & N=98  & 169.0 s (53.0) & 8.2 (2.6) & 7 - 10 & 0.810 \\
    MaRs-IB SF-3 & N=99  & 178.5 s (55.7) & 7.9 (2.7) & 6 - 10 & 0.829 \\
    RPM SF-A     & N=300 & 124.6 s (40.1) & 4.5 (2.0) & 3 - 9 & 0.791 \\
    \bottomrule
    \end{tabular*}
    \caption{\label{table:3}\normalfont Summary of performance on the MaRs-IB and Raven's progressive matrices (RPM) short form (SF) measures. IQR = interquartile range.}
\end{table}


\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{figures/fig00.png}
\caption{\label{fig:fig00} Example items from the MaRs-IB. (A) A simple item containing one element per cell and two rules (i.e. a change of shape and position). (B) A harder item containing three elements and six rules (i.e. three position changes, two color changes, one shape change). (C/D) Alternate versions of Items A and B, respectively, matched on complexity (i.e. number of elements and rules). In all panels, the first option is the correct response.}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{figures/fig01.png}
\caption{\label{fig:fig01}Summary of performance on the MaRs-IB items. (A) The distribution of total scores across all 1501 participants. (B) The distribution of proportion correct responses across all 384 items. (C) The distribution of participants' median response times across items broken down by participants' total scores and item difficulty (one minus proportion correct responses). Error bars indicate bootstrapped 95\% confidence intervals around the mean.}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{figures/fig02.png}
\caption{\label{fig:fig02}}
\end{figure}
 
\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{figures/fig03.png}
\caption{\label{fig:fig03}Results from the test assembly of three parallel MaRs-IB short form (SF) measures. (A) The psychometric properties of the items selected for each form, compared to all remaining items. Points represent the posterior means of the item difficulty and discrimination  parameters. (B) The test characteristic curves (TCCs) for each short form. The degree of overlap highlights the similarity of expected scores for each test form. (C) The test information functions (TIFs) for each short form. The degree of overlap highlights the similarity of reliability for each test form.}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{figures/fig04.png}
\caption{\label{fig:fig04}The joint distribution of total scores on the MaRs-IB short form and abbreviated Raven's progressive matrices (RPM) measures. The distribution of observed scores for each measure are shown in the margins.}
\end{figure}

\end{document}
